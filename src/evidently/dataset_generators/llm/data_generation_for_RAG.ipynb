{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "697ef555-f62c-424f-90da-bec9fbdace28",
   "metadata": {},
   "source": [
    "## Extra Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf4855a8-0d91-4d88-8fa2-05d2eb2ddbad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (0.5.12)\n",
      "Requirement already satisfied: build>=1.0.3 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from chromadb) (1.2.1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from chromadb) (2.9.2)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from chromadb) (0.115.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.31.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from chromadb) (1.25.2)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from chromadb) (3.6.6)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from chromadb) (1.19.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from chromadb) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from chromadb) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from chromadb) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from chromadb) (1.26.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from chromadb) (0.20.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from chromadb) (4.66.5)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from chromadb) (6.4.5)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from chromadb) (1.66.2)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from chromadb) (4.2.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from chromadb) (0.9.4)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from chromadb) (31.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from chromadb) (8.5.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from chromadb) (5.0.1)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from chromadb) (3.10.7)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from chromadb) (0.27.2)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from chromadb) (13.9.1)\n",
      "Requirement already satisfied: packaging>=19.1 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from build>=1.0.3->chromadb) (24.1)\n",
      "Requirement already satisfied: pyproject_hooks in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb) (0.38.6)\n",
      "Requirement already satisfied: anyio in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (4.6.0)\n",
      "Requirement already satisfied: certifi in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (1.0.6)\n",
      "Requirement already satisfied: idna in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: sniffio in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.35.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
      "Requirement already satisfied: durationpy>=0.7 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: coloredlogs in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.5)\n",
      "Requirement already satisfied: sympy in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=8.0.0,>=6.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.65.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.26.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.26.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.47b0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.47b0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.47b0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.47b0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.47b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (65.5.1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from opentelemetry-instrumentation-asgi==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (2.23.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from tokenizers>=0.13.2->chromadb) (0.25.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.20.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.24.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (13.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.12.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.9.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from importlib-metadata<=8.0.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.3.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.2\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3.11 -m pip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b8aec3-e00e-4919-b2a8-b19722311261",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "547c43f3-e58f-450c-b80b-c396eb2655a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai \n",
    "from openai import OpenAI\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f649ddb-af77-4961-8eb2-f69b7c6916db",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d8da19c-8d15-467f-90d0-dc7e02ed3aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chromadb.api.client.Client at 0x2bc6254d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6fac99-e1db-48be-9554-88ecddac271e",
   "metadata": {},
   "source": [
    "## Chunked data collection setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b93b470-9d32-4757-9d03-915992e2a7c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svetlanapopova/.pyenv/versions/3.11.9/envs/env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection db_collection created successfully\n",
      "{'data': None,\n",
      " 'documents': [],\n",
      " 'embeddings': None,\n",
      " 'ids': [],\n",
      " 'included': ['metadatas', 'documents'],\n",
      " 'metadatas': [],\n",
      " 'uris': None}\n"
     ]
    }
   ],
   "source": [
    "collection_name = \"db_collection\"\n",
    "default_embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chromadb/\")\n",
    "\n",
    "# declare ChromaDB collection\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=collection_name,\n",
    "    embedding_function=default_embedding_function\n",
    "    )\n",
    "\n",
    "result = collection.get()\n",
    "\n",
    "print(f\"Collection {collection_name} created successfully\")\n",
    "pprint.pprint(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d547021f-9d4d-42cf-b580-abc6a1008cd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_md_from_dir(dir_path):\n",
    "    \"\"\"\n",
    "    Loads Markdown (.md) files from the specified directory.\n",
    "\n",
    "    Args:\n",
    "        dir_path (str): Path to the directory containing .md files.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: A list of dictionaries with the text content of each .md file.\n",
    "    \"\"\"\n",
    "    md_files = [\n",
    "        os.path.join(dir_path, filename) \n",
    "        for filename in os.listdir(dir_path) \n",
    "        if filename.endswith(\".md\")\n",
    "    ]\n",
    "    \n",
    "    documents = []\n",
    "    for file_path in md_files:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            documents.append({\"text\": file.read()})\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53f147f1-f8a2-4095-a840-2bacbc0aaf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, chunk_size=100, chunk_overlap=20):\n",
    "    \"\"\"\n",
    "    Splits the input text into overlapping chunks.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to split.\n",
    "        chunk_size (int): The size of each chunk. Default is 100.\n",
    "        chunk_overlap (int): The number of overlapping characters between chunks. Default is 20.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    text_length = len(text)\n",
    "    \n",
    "    for start in range(0, text_length, chunk_size - chunk_overlap):\n",
    "        end = min(start + chunk_size, text_length)\n",
    "        chunks.append(text[start:end])\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0c9c8e2-0f2f-4fe0-aeee-68b0bb67cea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 files loaded\n",
      "Split in to 8 chunks\n"
     ]
    }
   ],
   "source": [
    "directory_path = \"../../../../evidently_reference/\"\n",
    "\n",
    "# load documents from directory\n",
    "md_files = load_md_from_dir(directory_path)\n",
    "\n",
    "print(f\" {len(md_files)} files loaded\")\n",
    "\n",
    "# Split text into chunks\n",
    "chunked_files = [\n",
    "    {\n",
    "        'id': f\"{file_id}-{chunk_id}\",\n",
    "        'text': chunk,\n",
    "    }\n",
    "    for file_id, file in enumerate(md_files)\n",
    "    for chunk_id, chunk in enumerate(split_text(file[\"text\"], chunk_size=500, chunk_overlap=50))\n",
    "]\n",
    "\n",
    "print(f\"Split in to {len(chunked_files)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bf99fde-8aa7-4111-ad7e-eec59bd0c23e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunked_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# insert documents with embeddings to collection ChromaDB\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[43mchunked_files\u001B[49m:\n\u001B[1;32m      3\u001B[0m     collection\u001B[38;5;241m.\u001B[39mupsert(\n\u001B[1;32m      4\u001B[0m             ids\u001B[38;5;241m=\u001B[39mchunk[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m      5\u001B[0m             documents\u001B[38;5;241m=\u001B[39mchunk[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m      6\u001B[0m     )\n\u001B[1;32m      8\u001B[0m result \u001B[38;5;241m=\u001B[39m collection\u001B[38;5;241m.\u001B[39mget()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'chunked_files' is not defined"
     ]
    }
   ],
   "source": [
    "# insert documents with embeddings to collection ChromaDB\n",
    "for chunk in chunked_files:\n",
    "    collection.upsert(\n",
    "            ids=chunk['id'],\n",
    "            documents=chunk['text'],\n",
    "    )\n",
    "\n",
    "result = collection.get()\n",
    "\n",
    "print(f\"Collection {collection_name} has {len(result['ids'])} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50ec8822-e2dc-4a01-bad3-44f1f123ed5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Collection(id=639fbee3-87d9-447e-a134-f2ad8596b07c, name=db_collection)]\n"
     ]
    }
   ],
   "source": [
    "#Just incase we need to delete collection\n",
    "list_collections = chroma_client.list_collections()\n",
    "print(list_collections)\n",
    "\n",
    "#chroma_client.delete_collection(collection_name)\n",
    "#list_collections = chroma_client.list_collections()\n",
    "#print(list_collections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaf34e6-d454-4d38-b08a-f57550b39e74",
   "metadata": {},
   "source": [
    "## Dataset Generation chain of promts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ced14436-c4b3-4b25-8cc7-7cdb112eed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b0cf1e-5356-44e4-855b-5169a21260e2",
   "metadata": {},
   "source": [
    "### Naive questions generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d81f6108-79ef-4148-bc8e-b1050cc1637f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ity Metrics than included in the `DataQualityPreset`. \\n\\n# How to read the tables\\n\\n* **Name**: the name of the Metric.  \\n* **Description**: plain text explanation. For Metrics, we also specify whether it applies to the whole dataset or individual columns.\\n* **Parameters**: required and optional parameters for the Metric or Preset. We also specify the defaults that apply if you do not pass a custom parameter.\\n\\n**Metric visualizations**. Each Metric includes a default render. To see the visualizati',\n",
       " 'r><br> | **Required**:<ul><li>`k`</li></ul>**Optional**:<ul><li>-</li></ul> |\\n| **PopularityBias()** <br><br> Evaluates the popularity bias in recommendations by computing ARP (average recommendation popularity), Gini index, and coverage. <br><br>Requires a training dataset. | **Required**:<ul><li>`K`</li><li>`normalize_arp (default: False)` - whether to normalize ARP calculation by the most popular item in training</li></ul>**Optional**:<ul><li>-</li></ul> |\\n| **ItemBiasMetric()** <br><br> Visu',\n",
       " 'th reference**: the test fails if the TNR is over 20% higher or lower.<br><br>**No reference**: the test fails if the TNR is lower than the TNR of the dummy model. |\\n| **TestFPR()** | Dataset-level. <br><br> Computes the False Positive Rate and compares it to the reference or against a defined condition. | **Required**:<br>N/A<br><br> **Optional:**<ul><li>`probas_threshold`(default for classification = None; default for probabilistic classification = 0.5)</li><li>`k` (default = None)</li></ul>**',\n",
       " 'tems by a chosen characteristic.\\n\\nThe visualization shows:\\n* The distribution of items in the training set for the defined `column_name` (with duplicates dropped). This represents the item catalog by this dimension. \\n* The distribution of the recommended items for the defined `column_name` in the current and reference (if available) datasets. \\n\\nThis visualization helps see the patterns in the model recommendations. In a simplified example, you might observe that the training data contains 3x com',\n",
       " '.|\\n| **TestGiniIndex(k=k)** | Dataset-level. <br><br> Computes the Gini Index at the top K recommendations and compares it to the reference or against a defined condition.<br><br>Requires a training dataset. | **Required**:<ul><li>`k`</li></ul> **Optional**:<br>N/A<br><br> **Test conditions**: <ul><li>*standard parameters*</li></ul> | Expects +/-10% from reference.<br><br>**With reference**: if the Gini Index at the top K is over 10% higher or lower, the test fails.<br><br>**No reference**: Test',\n",
       " ' a defined condition. | **Required**:<br>N/A<br><br> **Optional:**<ul><li>`probas_threshold`(default for classification = None; default for probabilistic classification = 0.5)</li><li>`k` (default = None)</li></ul>**Test conditions**: <ul><li>*standard parameters*</li></ul>| Expects +/-20% or better than a dummy model.<br><br>**With reference**: the test fails if the FNR is over 20% higher or lower.<br><br>**No reference**: the test fails if the FNR is higher than the FNR of the dummy model. |\\n|',\n",
       " ' test fails if the dataset contains rows with missing values.|\\n| **TestShareOfRowsWithMissingValues()** | Dataset-level. <br><br> Tests the share of rows that contain missing values against the reference or a defined condition. | **Required**:<br>N/A<br><br>**Optional**:<ul><li>`missing_values = [], replace = True/False` (default = default list)</li></ul>**Test conditions** <ul><li>*standard parameters*</li></ul>| Expects up to +10% or 0.<br><br>**With reference**: the test fails if the share of',\n",
       " 'rcentage error in a line plot. | **Required:**<br>n/a<br><br>**Optional:**<br>n/a |\\n| **RegressionErrorDistribution()** <br><br> Visualizes the distribution of the model error in a histogram. | **Required:**<br>n/a<br><br>**Optional:**<br>n/a |\\n| **RegressionErrorNormality()** <br><br> Visualizes the quantile-quantile plot (Q-Q plot) to estimate value normality. | **Required:**<br>n/a<br><br>**Optional:**<br>n/a |\\n| **RegressionTopErrorMetric()** <br><br> Calculates the regression performance me',\n",
       " \"\\n**Note**: Only a single top relevant item is considered in this metric, disregarding the position and relevance of other items in the list.\\n\\n# Diversity\\n\\n![](../.gitbook/assets/reports/metric_diversity-min.png)\\n\\n**Evidently Metric**: `DiversityMetric`\\n\\n**Recommendation diversity**: this metric measures the average intra-list diversity at K. It reflects the variety of items within the same user's recommendation list, averaged by all users. \\n\\n**Implemented method**:\\n* **Measure the difference bet\",\n",
       " '*: the test fails if there is at least one empty column.|\\n| **TestNumberOfDuplicatedRows()** | Dataset-level. <br><br> Tests the number of duplicate rows against reference or a defined condition. |**Required**:<br> N/A <br><br> **Optional**:<br> N/A <br><br>**Test conditions**: <ul><li>*standard parameters*</li></ul>| Expects +/- 10% or none.<br><br>**With reference**: the test fails if the share of duplicate rows is over 10% higher or lower than in the reference.<br><br>**No reference**: the te']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fixed size for the random list\n",
    "sample_size = 10\n",
    "\n",
    "# Generate a random list with the fixed size from the existing list\n",
    "random_chuncks = [item['text'] for item in random.sample(chunked_files, min(sample_size, len(chunked_files)))]\n",
    "random_chuncks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90712120-3ac4-48d0-a749-9f8ef72f4247",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an assisstant who generates questions based on provided context\"\n",
    "number_of_questions = 10\n",
    "user_prompt = \"\"\"\n",
    "Generate {N} conceptual questions based on the provided context and can be answered from the information in the provided context.\n",
    "Here is a context\n",
    "<context>\n",
    "    {context}\n",
    "</context>\n",
    "\n",
    "Remain faithful to the underlying context. \n",
    "Avoid providing any preamble!\n",
    "Avoid providing any closing statement!\n",
    "Please return only a list of coma separated generated questions in string format.\n",
    "\"\"\"\n",
    "\n",
    "context = \"\\n\\n\".join(random_chuncks)\n",
    "\n",
    "formated_user_prompt = user_prompt.format(context=context, N=number_of_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7aa2632f-5bda-4395-81a2-77ccb4dd994b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",  # Updated to a valid model\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": formated_user_prompt}\n",
    "    ],\n",
    "    max_tokens=400,  # Limits the response length\n",
    "    temperature=0.7,  # Controls randomness in the output\n",
    "    n=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d01f1b79-7781-4e60-b6b0-71a31f860376",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_queries = response.choices[0].message.content.strip().split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ba9d23b-b502-4e6e-9535-6c672d6ec309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"How is the \\'Name\\' of a Metric used in reading tables?',\n",
       " \" What information does the 'Description' section provide in the context of Metrics?\",\n",
       " \" What parameters are considered for the 'PopularityBias()' Metric?\",\n",
       " \" How does the 'TestFPR()' function operate at the dataset level?\",\n",
       " \" What is the role of the 'TestGiniIndex(k=k)' in evaluating dataset bias?\",\n",
       " \" What visualization is provided by the 'RegressionErrorDistribution()'?\",\n",
       " \" How does the 'RegressionErrorNormality()' assess value normality?\",\n",
       " \" What is the primary focus of the 'DiversityMetric' in recommendation systems?\",\n",
       " \" How are missing values tested in the 'TestShareOfRowsWithMissingValues()'?\",\n",
       " ' How does the \\'TestNumberOfDuplicatedRows()\\' function evaluate dataset integrity?\"']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8547f2-b3a7-4058-8175-b3872f318d1a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### [PLEASE IGNORE THE WHOLE BLOCK] Get alternative questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe13a2c7-1c76-4d18-8bde-d1821078822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#it is not used so far\n",
    "seed_query = \"How do I get Evidently data drift report for my data?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d135ad3-5be4-45de-8039-c556770c32c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seed question generation\n",
    "system_prompt = \"You are an assisstant who generates questions based on provided context\"\n",
    "user_prompt = \"\"\"\n",
    "Generate a conceptual question based on the provided context and can be answered from the information in the provided context.\n",
    "Here is a context\n",
    "<context>\n",
    "    {context}\n",
    "</context>\n",
    "\n",
    "Remain faithful to the underlying context. \n",
    "Avoid providing any preamble!\n",
    "Avoid providing any closing statement!\n",
    "Please return only a question\n",
    "\"\"\"\n",
    "\n",
    "context = \"\\n\\n\".join(random_chuncks)\n",
    "\n",
    "formated_user_prompt = user_prompt.format(context=context, N=number_of_questions)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",  # Updated to a valid model\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": formated_user_prompt}\n",
    "    ],\n",
    "    max_tokens=400,  # Limits the response length\n",
    "    temperature=0.7,  # Controls randomness in the output\n",
    "    n=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b751174-f5ab-4b2c-bde1-3e1c9bcc9d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_seed = response.choices[0].message.content.strip().split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82c0618f-e790-4565-acad-e62edf4dfba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How does the `PopularityBias()` metric evaluate recommendation systems',\n",
       " ' and what parameters are required to compute this metric?']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39df8c68-84cb-43af-aba3-63d1f10537ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do not forget to write a prompt for seed query generation\n",
    "system_prompt = \"You are a smart assistant who helps rephrase questions\" \n",
    "\n",
    "number_of_reformulations = 5\n",
    "\n",
    "seed_query = generated_seed\n",
    "\n",
    "user_prompt = \"\"\"Write for me {number_of_reformulations} alternative questions quite similar to the question you got.\n",
    "The question: {seed_query}\n",
    "\n",
    "Return a list of questions.\n",
    "This should be only a list of string questions, separated by comma\n",
    "\"\"\"\n",
    "\n",
    "formated_user_prompt = user_prompt.format(number_of_reformulations=number_of_reformulations, \n",
    "                                          seed_query=seed_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "983b4545-0511-473e-8797-7fbdf2d5ff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a request to the OpenAI to expand a seed question\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",  # Updated to a valid model\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": formated_user_prompt}\n",
    "    ],\n",
    "    max_tokens=400,  # Limits the response length\n",
    "    temperature=0.7,  # Controls randomness in the output\n",
    "    n=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c2fe61b-5470-469a-949c-9e1a65c0f4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Completion:\n",
      "['What parameters are needed to calculate the `PopularityBias()` metric in evaluating recommendation systems?', 'In what way does the `PopularityBias()` metric assess recommendation systems, and what are the necessary parameters?', 'Which parameters are essential for the `PopularityBias()` metric, and how does it evaluate recommendation systems?', 'How is the `PopularityBias()` metric used to evaluate recommendation systems, and what parameters does it need?', 'What is the role of the `PopularityBias()` metric in assessing recommendation systems, and which parameters are required for its computation?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"['What parameters are needed to calculate the `PopularityBias()` metric in evaluating recommendation systems?'\",\n",
       " \" 'In what way does the `PopularityBias()` metric assess recommendation systems\",\n",
       " \" and what are the necessary parameters?'\",\n",
       " \" 'Which parameters are essential for the `PopularityBias()` metric\",\n",
       " \" and how does it evaluate recommendation systems?'\",\n",
       " \" 'How is the `PopularityBias()` metric used to evaluate recommendation systems\",\n",
       " \" and what parameters does it need?'\",\n",
       " \" 'What is the role of the `PopularityBias()` metric in assessing recommendation systems\",\n",
       " \" and which parameters are required for its computation?']\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion_text = response.choices[0].message.content\n",
    "print(f\"Generated Completion:\\n{completion_text}\")\n",
    "\n",
    "queries = completion_text.strip().split(\",\")\n",
    "queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df169dc5-acbd-46e7-b163-c5ebebb8ea0d",
   "metadata": {},
   "source": [
    "### Find relevant chuncks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0333932a-8f6e-48e6-9f28-9f5c0406d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_collection(question, n_results = 3):\n",
    "    \"\"\"\n",
    "    Queries the collection with a given question and returns the relevant text chunks.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The query or question text to search for.\n",
    "        n_results (int): Number of results to retrieve. Default is 3.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of relevant text chunks.\n",
    "    \"\"\"\n",
    "    # Perform the query\n",
    "    results = collection.query(\n",
    "        query_texts=question,\n",
    "        n_results=n_results,\n",
    "        # include=['embeddings', 'documents', 'distances']\n",
    "    )\n",
    "\n",
    "    # Extract relevant text chunks from the documents\n",
    "    relevant_chunks = [\n",
    "        chunk for document in results[\"documents\"] for chunk in document\n",
    "    ]\n",
    "    \n",
    "    return relevant_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b827e30d-a7b2-406f-a139-5b7fdd3bab6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' times item *i* was rated in the training set (popularity of item *i*)\\n\\n**Range**: 0 to infinity \\n\\n**Interpretation**: the higher the value, the more popular on average the recommendations are in top-K.  \\n\\n**Note**: This metric is not normalized and depends on the number of recommendations in the training set.\\n\\nFurther reading: [Abdollahpouri, H., Mansoury, M., Burke, R., Mobasher, B., & Malthouse, E. (2021). User-centered Evaluation of Popularity Bias in Recommender Systems](https://dl.acm.org/',\n",
       " 'bdollahpouri, H., Mansoury, M., Burke, R., Mobasher, B., & Malthouse, E. (2021). User-centered Evaluation of Popularity Bias in Recommender Systems](https://dl.acm.org/doi/fullHtml/10.1145/3450613.3456821)\\n\\n# Recommendation table\\n\\n![](../.gitbook/assets/reports/metric_recsys_table-min.png)\\n\\n**Evidently Metric**: `RecCasesTable`\\n\\nThis visual Metric shows the list of recommendations for the specified user IDs (`user_ids: List`). If you do not pass the list of IDs, Evidently will choose 5 random on',\n",
       " 'reports/metric_popularity_bias-min.png)\\n\\n**Evidently Metric**: `PopularityBias`\\n\\nThe recommendation popularity bias is a tendency to favor a few popular items. This metric includes several measurements: ARP, Coverage and Gini index.\\n\\n## 1. Average Recommendation Popularity (ARP)\\n\\nARP reflects the average popularity of the items recommended to the users. \\n\\n**Implementation**.\\n* Compute the item popularity as the number of times each item was seen in training. \\n* Compute the average popularity for',\n",
       " '---\\ndescription: List of Metrics, Descriptors and Metric Presets available in Evidently.\\n---\\n\\n<details>\\n\\n<summary>How to use this page</summary>\\n\\nThis is a reference page. It shows all the available Metrics, Descriptors and Presets. \\n  \\nYou can use the menu on the right to navigate the sections. We organize the Metrics by logical groups. Note that these groups do **not** match the Presets with a similar name. For example, there are more Data Quality Metrics than included in the `DataQualityPrese',\n",
       " 'ity Metrics than included in the `DataQualityPreset`. \\n\\n# How to read the tables\\n\\n* **Name**: the name of the Metric.  \\n* **Description**: plain text explanation. For Metrics, we also specify whether it applies to the whole dataset or individual columns.\\n* **Parameters**: required and optional parameters for the Metric or Preset. We also specify the defaults that apply if you do not pass a custom parameter.\\n\\n**Metric visualizations**. Each Metric includes a default render. To see the visualizati',\n",
       " 'unique <= 2): proportion difference test for independent samples based on Z-score.\\n\\nAll tests use a 0.95 confidence level by default.  \\n\\nFor **larger data with \\\\> 1000 observations** in the reference dataset:\\n\\n* For numerical columns (n\\\\_unique \\\\> 5):[Wasserstein Distance](https://en.wikipedia.org/wiki/Wasserstein_metric).\\n* For categorical columns or numerical with n\\\\_unique <= 5):[Jensen--Shannon divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence).\\n\\nAll metrics use a t']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_collection(seed_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d549b9ab-1e3a-490e-a57e-669af72dbdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#relevant_chunks = [query_collection(query) for query in queries]\n",
    "relevant_chunks = [query_collection(query) for query in generated_queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cfeb5073-f37c-4d24-85a7-bf2043dacb1e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ity Metrics than included in the `DataQualityPreset`. \\n\\n# How to read the tables\\n\\n* **Name**: the name of the Metric.  \\n* **Description**: plain text explanation. For Metrics, we also specify whether it applies to the whole dataset or individual columns.\\n* **Parameters**: required and optional parameters for the Metric or Preset. We also specify the defaults that apply if you do not pass a custom parameter.\\n\\n**Metric visualizations**. Each Metric includes a default render. To see the visualizati',\n",
       "  'igate the sections. \\n\\n# How to read the tables\\n\\n* **Name**: the name of the Test or Test preset.  \\n* **Description**: plain text explanation. For Tests, we specify whether it applies to the whole dataset or individual columns.\\n* **Parameters**: available configurations. \\n  * Required parameters are necessary for calculations, e.g. a column name for a column-level test.\\n  * Optional parameters modify how the underlying metric is calculated, e.g. which statistical test or correlation method is use',\n",
       "  '---\\ndescription: List of Metrics, Descriptors and Metric Presets available in Evidently.\\n---\\n\\n<details>\\n\\n<summary>How to use this page</summary>\\n\\nThis is a reference page. It shows all the available Metrics, Descriptors and Presets. \\n  \\nYou can use the menu on the right to navigate the sections. We organize the Metrics by logical groups. Note that these groups do **not** match the Presets with a similar name. For example, there are more Data Quality Metrics than included in the `DataQualityPrese'],\n",
       " ['---\\ndescription: List of Metrics, Descriptors and Metric Presets available in Evidently.\\n---\\n\\n<details>\\n\\n<summary>How to use this page</summary>\\n\\nThis is a reference page. It shows all the available Metrics, Descriptors and Presets. \\n  \\nYou can use the menu on the right to navigate the sections. We organize the Metrics by logical groups. Note that these groups do **not** match the Presets with a similar name. For example, there are more Data Quality Metrics than included in the `DataQualityPrese',\n",
       "  'ity Metrics than included in the `DataQualityPreset`. \\n\\n# How to read the tables\\n\\n* **Name**: the name of the Metric.  \\n* **Description**: plain text explanation. For Metrics, we also specify whether it applies to the whole dataset or individual columns.\\n* **Parameters**: required and optional parameters for the Metric or Preset. We also specify the defaults that apply if you do not pass a custom parameter.\\n\\n**Metric visualizations**. Each Metric includes a default render. To see the visualizati',\n",
       "  '*Interpretation**: the higher the value, the more varied items are shown to each user (e.g. inside a single recommendation block).\\n\\n**Requirements**: You must pass the `item_features` list to point to numerical columns or embeddings that describe the recommended items. For example, these could be encoded genres that represent each movie. This makes it possible to compare the degree of similarity between different items. \\n\\n**Notes**: \\n* This metric does not consider relevance. A recommender syste'],\n",
       " ['reports/metric_popularity_bias-min.png)\\n\\n**Evidently Metric**: `PopularityBias`\\n\\nThe recommendation popularity bias is a tendency to favor a few popular items. This metric includes several measurements: ARP, Coverage and Gini index.\\n\\n## 1. Average Recommendation Popularity (ARP)\\n\\nARP reflects the average popularity of the items recommended to the users. \\n\\n**Implementation**.\\n* Compute the item popularity as the number of times each item was seen in training. \\n* Compute the average popularity for',\n",
       "  ' times item *i* was rated in the training set (popularity of item *i*)\\n\\n**Range**: 0 to infinity \\n\\n**Interpretation**: the higher the value, the more popular on average the recommendations are in top-K.  \\n\\n**Note**: This metric is not normalized and depends on the number of recommendations in the training set.\\n\\nFurther reading: [Abdollahpouri, H., Mansoury, M., Burke, R., Mobasher, B., & Malthouse, E. (2021). User-centered Evaluation of Popularity Bias in Recommender Systems](https://dl.acm.org/',\n",
       "  'r><br> | **Required**:<ul><li>`k`</li></ul>**Optional**:<ul><li>-</li></ul> |\\n| **PopularityBias()** <br><br> Evaluates the popularity bias in recommendations by computing ARP (average recommendation popularity), Gini index, and coverage. <br><br>Requires a training dataset. | **Required**:<ul><li>`K`</li><li>`normalize_arp (default: False)` - whether to normalize ARP calculation by the most popular item in training</li></ul>**Optional**:<ul><li>-</li></ul> |\\n| **ItemBiasMetric()** <br><br> Visu'],\n",
       " ['th reference**: the test fails if the TNR is over 20% higher or lower.<br><br>**No reference**: the test fails if the TNR is lower than the TNR of the dummy model. |\\n| **TestFPR()** | Dataset-level. <br><br> Computes the False Positive Rate and compares it to the reference or against a defined condition. | **Required**:<br>N/A<br><br> **Optional:**<ul><li>`probas_threshold`(default for classification = None; default for probabilistic classification = 0.5)</li><li>`k` (default = None)</li></ul>**',\n",
       "  'In some tests and metrics, Evidently uses the default Data Drift Detection algorithm. It helps detect the distribution drift in the individual features, prediction, or target. This page describes how the **default** algorithm works.\\n\\n# How it works\\n\\nEvidently compares the distributions of the values in a given column (or columns) of the two datasets. You should pass these datasets as **reference** and **current**. Evidently applies several statistical tests and drift detection methods to detect ',\n",
       "  ' parameters**:\\n* `columns`\\n\\n</details>\\n\\n<details>\\n\\n<summary>Data Drift Preset</summary>\\n\\n`DataDriftPreset` evaluates the data distribution drift in all individual columns, and share of drifting columns in the dataset. Input columns are required. \\n\\n**Composition**:\\n* `DataDriftTable()` for all or specified `columns`\\n* `DatasetDriftMetric()` for all or specified `columns`\\n\\n**Optional parameters**:\\n* `columns`\\n* `stattest`\\n* `cat_stattest`\\n* `num_stattest`\\n* `per_column_stattest`\\n* `text_stattest`\\n'],\n",
       " ['wer, the test fails.<br><br>**No reference**: Tests if Gini Index < 1. |\\n| **TestCoverage(k=k)** | Dataset-level. <br><br> Computes the Coverage at the top K recommendations and compares it to the reference or against a defined condition. <br><br>Requires a training dataset. | **Required**:<ul><li>`k`</li></ul> **Optional**:<br>N/A<br><br>**Test conditions**: <ul><li>*standard parameters*</li></ul> | Expects +/-10% from reference.<br><br>**With reference**: if the Coverage at the top K is over 1',\n",
       "  '.|\\n| **TestGiniIndex(k=k)** | Dataset-level. <br><br> Computes the Gini Index at the top K recommendations and compares it to the reference or against a defined condition.<br><br>Requires a training dataset. | **Required**:<ul><li>`k`</li></ul> **Optional**:<br>N/A<br><br> **Test conditions**: <ul><li>*standard parameters*</li></ul> | Expects +/-10% from reference.<br><br>**With reference**: if the Gini Index at the top K is over 10% higher or lower, the test fails.<br><br>**No reference**: Test',\n",
       "  'eference**: if the Coverage at the top K is over 10% higher or lower, the test fails.<br><br>**No reference**: Tests if Coverage > 0.|\\n\\n'],\n",
       " ['ter plot. | **Required:**<br>n/a<br><br>**Optional:**<br>n/a |\\n| **RegressionPredictedVsActualPlot()** <br><br> Visualizes predicted vs. actual values in a line plot. | **Required:**<br>n/a<br><br>**Optional:**<br>n/a |\\n| **RegressionErrorPlot()** <br><br> Visualizes the model error (predicted - actual) in a line plot. | **Required:**<br>n/a<br><br>**Optional:**<br>n/a |\\n| **RegressionAbsPercentageErrorPlot()** <br><br> Visualizes the absolute percentage error in a line plot. | **Required:**<br>',\n",
       "  'rcentage error in a line plot. | **Required:**<br>n/a<br><br>**Optional:**<br>n/a |\\n| **RegressionErrorDistribution()** <br><br> Visualizes the distribution of the model error in a histogram. | **Required:**<br>n/a<br><br>**Optional:**<br>n/a |\\n| **RegressionErrorNormality()** <br><br> Visualizes the quantile-quantile plot (Q-Q plot) to estimate value normality. | **Required:**<br>n/a<br><br>**Optional:**<br>n/a |\\n| **RegressionTopErrorMetric()** <br><br> Calculates the regression performance me',\n",
       "  'all scores, if available). \\n\\n# Item Bias \\n\\n![](../.gitbook/assets/reports/metric_itembias_2-min.png)\\n\\n**Evidently Metric**: `ItemBiasMetric`\\n\\nThis visual metric shows the distribution of recommendations by a specified category or numerical value (`column_name`) compared to its distribution in the training set.\\n\\nThis helps compare the model recommendations against what could have been a random classifier that follows the observed distribution of items by a chosen characteristic.\\n\\nThe visualizatio'],\n",
       " ['rcentage error in a line plot. | **Required:**<br>n/a<br><br>**Optional:**<br>n/a |\\n| **RegressionErrorDistribution()** <br><br> Visualizes the distribution of the model error in a histogram. | **Required:**<br>n/a<br><br>**Optional:**<br>n/a |\\n| **RegressionErrorNormality()** <br><br> Visualizes the quantile-quantile plot (Q-Q plot) to estimate value normality. | **Required:**<br>n/a<br><br>**Optional:**<br>n/a |\\n| **RegressionTopErrorMetric()** <br><br> Calculates the regression performance me',\n",
       "  'her than the RMSE of the dummy model that predicts the optimal constant (mean of the target value). |\\n| **TestValueMeanError()**<br>| Dataset-level. <br><br> Computes the Mean Error (ME) and tests if it is near zero or compares it against a defined condition. | **Required**:<br>N/A<br><br> **Optional**:<br>N/A<br><br> **Test conditions** <ul><li>*standard parameters*</li></ul>| Expects the Mean Error to be near zero.<br><br>**With/without reference**: the test fails if the Mean Error is skewed a',\n",
       "  'n and underestimation).</li></ul> |\\n| **RegressionErrorBiasTable()** <br><br> Plots the relationship between feature values and model quality per group (for top-X% error groups, as above). | **Required:**<br>n/a<br><br>**Optional:**<ul><li>`columns`(default = all categorical and numerical columns)</li><li>`top_error` (default=0.05; the metrics are calculated for top-5% predictions with overestimation and underestimation).</li></ul>|\\n\\n# Ranking and Recommendations \\n\\nAll metrics are dataset-level.'],\n",
       " [\"\\n**Note**: Only a single top relevant item is considered in this metric, disregarding the position and relevance of other items in the list.\\n\\n# Diversity\\n\\n![](../.gitbook/assets/reports/metric_diversity-min.png)\\n\\n**Evidently Metric**: `DiversityMetric`\\n\\n**Recommendation diversity**: this metric measures the average intra-list diversity at K. It reflects the variety of items within the same user's recommendation list, averaged by all users. \\n\\n**Implemented method**:\\n* **Measure the difference bet\",\n",
       "  \"\\n* **Intra-list diversity**. Calculate intra-list diversity for each user by averaging the Cosine Distance between each pair of items in the user's top-K list.\\n* **Overall diversity**. Calculate the overall diversity by averaging the intra-list diversity across all users.\\n\\n**Range**: The metric is based on Cosine distance, and can take values from 0 to 2. \\n**0:** identical recommendations in top-K.\\n**2:** very diverse recommendations in top-K.\\n\\n**Interpretation**: the higher the value, the more \",\n",
       "  'ders items that are present in training. \\n\\nFurther reading: [Castells, P., Vargas, S., & Wang, J. (2011). Novelty and Diversity Metrics for Recommender Systems: Choice, Discovery and Relevance](https://repositorio.uam.es/bitstream/handle/10486/666094/novelty_castells_DDR_2011.pdf)\\n\\n# Serendipity\\n\\n![](../.gitbook/assets/reports/metric_serendipity-min.png)\\n\\n**Evidently Metric**: `SerendipityMetric`\\n\\nRecommendation serendipity: this metric measures how unusual the relevant recommendations are in K,'],\n",
       " ['r> **Optional**: <ul><li>`missing_values = [], replace = True/False` (default = default list)</li></ul> **Test conditions**: <ul><li>*standard parameters*</li></ul>| Expects up to +10% or 0. <br><br>**With reference**: the test fails if the share of missing values is over 10% higher than in reference. <br><br>**No reference**: the test fails if the dataset contains missing values.|\\n| **TestShareOfMissingValues()**| Dataset-level. <br><br> Tests the share of missing values in the dataset against ',\n",
       "  ' test fails if the dataset contains rows with missing values.|\\n| **TestShareOfRowsWithMissingValues()** | Dataset-level. <br><br> Tests the share of rows that contain missing values against the reference or a defined condition. | **Required**:<br>N/A<br><br>**Optional**:<ul><li>`missing_values = [], replace = True/False` (default = default list)</li></ul>**Test conditions** <ul><li>*standard parameters*</li></ul>| Expects up to +10% or 0.<br><br>**With reference**: the test fails if the share of',\n",
       "  '**With reference**: the test fails if the share of rows with missing values is over 10% higher than in reference. <br><br>**No reference**: the test fails if the dataset contains rows with missing values.|\\n| **TestNumberOfDifferentMissingValues()**| Dataset-level. <br><br> Tests the number of differently encoded missing values in the dataset against the reference or a defined condition. Detects 4 types of missing values by default and/or values from a user list. | **Required**:<br>N/A<br><br>**O'],\n",
       " ['*: the test fails if there is at least one empty column.|\\n| **TestNumberOfDuplicatedRows()** | Dataset-level. <br><br> Tests the number of duplicate rows against reference or a defined condition. |**Required**:<br> N/A <br><br> **Optional**:<br> N/A <br><br>**Test conditions**: <ul><li>*standard parameters*</li></ul>| Expects +/- 10% or none.<br><br>**With reference**: the test fails if the share of duplicate rows is over 10% higher or lower than in the reference.<br><br>**No reference**: the te',\n",
       "  '**With reference**: the test fails if the share of rows with missing values is over 10% higher than in reference. <br><br>**No reference**: the test fails if the dataset contains rows with missing values.|\\n| **TestNumberOfDifferentMissingValues()**| Dataset-level. <br><br> Tests the number of differently encoded missing values in the dataset against the reference or a defined condition. Detects 4 types of missing values by default and/or values from a user list. | **Required**:<br>N/A<br><br>**O',\n",
       "  ' in the reference.<br><br>**No reference**: the test fails if there is at least one duplicate row. |\\n| **TestNumberOfDuplicatedColumns()** | Dataset-level. <br><br> Tests the number of duplicate columns against reference or a defined condition. |**Required**:<br> N/A <br><br> **Optional**:<br> N/A <br><br>**Test conditions**: <ul><li>*standard parameters*</li></ul>| Expects =< or none.<br><br>**With reference**: the test fails if the number of duplicate columns is higher than in the reference.<b']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e24bdc4-fb5c-4df4-b00f-01903c4ed370",
   "metadata": {},
   "source": [
    "### Baseline answer generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee992257-020d-461b-9b2f-928b93acb4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a request to the OpenAI to answer generated question with relevant context\n",
    "\n",
    "def generate_baseline_answer(query, relevant_chunks):\n",
    "    system_prompt = \"You are a helpful assistant thet answer a given question directly withou any preamble\"\n",
    "\n",
    "    user_prompt = \"\"\"\n",
    "    Your task is to answer the following query: \n",
    "    <query>\n",
    "    {query}\n",
    "    </query>\n",
    "    \n",
    "    You have access to the following documents which are meant to provide context as you answer the query:\n",
    "    <documents>\n",
    "    {context}\n",
    "    </documents>\n",
    "    \n",
    "    Please remain faithful to the underlying context, and deviate from it only if you haven't found the answer in the provided context. \n",
    "    Avoid providing any preamble!\n",
    "    Avoid providing any closing statement!\n",
    "    Please return the answer only\n",
    "    \"\"\"\n",
    "    \n",
    "    context = \"\\n\\n\".join(relevant_chunks)\n",
    "    formated_user_prompt = user_prompt.format(query=query, context=context)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # Updated to a valid model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": formated_user_prompt}\n",
    "        ],\n",
    "        max_tokens=400,  # Limits the response length\n",
    "        temperature=0.7,  # Controls randomness in the output\n",
    "        n=1\n",
    "    )\n",
    "    \n",
    "    completion_text = response.choices[0].message.content\n",
    "    return completion_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8db672b0-f63a-400b-b00f-22e96d02dbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_answers = [generate_baseline_answer(generated_queries[i], relevant_chunks[i]) for i in range(min(len(generated_queries), len(relevant_chunks)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae366d95-3438-4d6c-8030-8e8c666e0e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_dataset = pd.DataFrame({\n",
    "    'Query': generated_queries,\n",
    "    'Relevant chunks': relevant_chunks,\n",
    "    'Baseline_answers': baseline_answers\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bdfc1029-34bb-4870-bcc2-0c32f56a0bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Relevant chunks</th>\n",
       "      <th>Baseline_answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"How is the 'Name' of a Metric used in reading...</td>\n",
       "      <td>[ity Metrics than included in the `DataQuality...</td>\n",
       "      <td>The 'Name' of a Metric is used to identify the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What information does the 'Description' secti...</td>\n",
       "      <td>[---\\ndescription: List of Metrics, Descriptor...</td>\n",
       "      <td>The 'Description' section provides a plain tex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What parameters are considered for the 'Popul...</td>\n",
       "      <td>[reports/metric_popularity_bias-min.png)\\n\\n**...</td>\n",
       "      <td>ARP, Coverage, and Gini index are the paramete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the 'TestFPR()' function operate at ...</td>\n",
       "      <td>[th reference**: the test fails if the TNR is ...</td>\n",
       "      <td>The `TestFPR()` function operates at the datas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the role of the 'TestGiniIndex(k=k)' ...</td>\n",
       "      <td>[wer, the test fails.&lt;br&gt;&lt;br&gt;**No reference**:...</td>\n",
       "      <td>The role of the 'TestGiniIndex(k=k)' in evalua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What visualization is provided by the 'Regres...</td>\n",
       "      <td>[ter plot. | **Required:**&lt;br&gt;n/a&lt;br&gt;&lt;br&gt;**Opt...</td>\n",
       "      <td>Visualizes the distribution of the model error...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How does the 'RegressionErrorNormality()' ass...</td>\n",
       "      <td>[rcentage error in a line plot. | **Required:*...</td>\n",
       "      <td>RegressionErrorNormality() assesses value norm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the primary focus of the 'DiversityMe...</td>\n",
       "      <td>[\\n**Note**: Only a single top relevant item i...</td>\n",
       "      <td>The primary focus of the 'DiversityMetric' in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How are missing values tested in the 'TestSha...</td>\n",
       "      <td>[r&gt; **Optional**: &lt;ul&gt;&lt;li&gt;`missing_values = []...</td>\n",
       "      <td>The 'TestShareOfRowsWithMissingValues()' tests...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How does the 'TestNumberOfDuplicatedRows()' f...</td>\n",
       "      <td>[*: the test fails if there is at least one em...</td>\n",
       "      <td>The 'TestNumberOfDuplicatedRows()' function ev...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Query  \\\n",
       "0  \"How is the 'Name' of a Metric used in reading...   \n",
       "1   What information does the 'Description' secti...   \n",
       "2   What parameters are considered for the 'Popul...   \n",
       "3   How does the 'TestFPR()' function operate at ...   \n",
       "4   What is the role of the 'TestGiniIndex(k=k)' ...   \n",
       "5   What visualization is provided by the 'Regres...   \n",
       "6   How does the 'RegressionErrorNormality()' ass...   \n",
       "7   What is the primary focus of the 'DiversityMe...   \n",
       "8   How are missing values tested in the 'TestSha...   \n",
       "9   How does the 'TestNumberOfDuplicatedRows()' f...   \n",
       "\n",
       "                                     Relevant chunks  \\\n",
       "0  [ity Metrics than included in the `DataQuality...   \n",
       "1  [---\\ndescription: List of Metrics, Descriptor...   \n",
       "2  [reports/metric_popularity_bias-min.png)\\n\\n**...   \n",
       "3  [th reference**: the test fails if the TNR is ...   \n",
       "4  [wer, the test fails.<br><br>**No reference**:...   \n",
       "5  [ter plot. | **Required:**<br>n/a<br><br>**Opt...   \n",
       "6  [rcentage error in a line plot. | **Required:*...   \n",
       "7  [\\n**Note**: Only a single top relevant item i...   \n",
       "8  [r> **Optional**: <ul><li>`missing_values = []...   \n",
       "9  [*: the test fails if there is at least one em...   \n",
       "\n",
       "                                    Baseline_answers  \n",
       "0  The 'Name' of a Metric is used to identify the...  \n",
       "1  The 'Description' section provides a plain tex...  \n",
       "2  ARP, Coverage, and Gini index are the paramete...  \n",
       "3  The `TestFPR()` function operates at the datas...  \n",
       "4  The role of the 'TestGiniIndex(k=k)' in evalua...  \n",
       "5  Visualizes the distribution of the model error...  \n",
       "6  RegressionErrorNormality() assesses value norm...  \n",
       "7  The primary focus of the 'DiversityMetric' in ...  \n",
       "8  The 'TestShareOfRowsWithMissingValues()' tests...  \n",
       "9  The 'TestNumberOfDuplicatedRows()' function ev...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3de32ca3-162f-4ed8-ba88-09a5b9572457",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db43a50d-4b1a-4b42-a529-67e85bef0f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Baseline_answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"How is the 'Name' of a Metric used in reading tables?</td>\n",
       "      <td>The 'Name' of a Metric is used to identify the specific Metric being referenced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What information does the 'Description' section provide in the context of Metrics?</td>\n",
       "      <td>The 'Description' section provides a plain text explanation of the Metric, specifying whether it applies to the whole dataset or individual columns.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What parameters are considered for the 'PopularityBias()' Metric?</td>\n",
       "      <td>ARP, Coverage, and Gini index are the parameters considered for the 'PopularityBias()' Metric.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the 'TestFPR()' function operate at the dataset level?</td>\n",
       "      <td>The `TestFPR()` function operates at the dataset level by computing the False Positive Rate (FPR) and comparing it to a reference or against a defined condition.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the role of the 'TestGiniIndex(k=k)' in evaluating dataset bias?</td>\n",
       "      <td>The role of the 'TestGiniIndex(k=k)' in evaluating dataset bias is to compute the Gini Index at the top K recommendations and compare it to a reference or a defined condition. If the Gini Index at the top K is over 10% higher or lower than the reference, the test fails. This helps in assessing the fairness and distribution of recommendations, indicating potential bias if the Gini Index significantly deviates from the reference.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What visualization is provided by the 'RegressionErrorDistribution()'?</td>\n",
       "      <td>Visualizes the distribution of the model error in a histogram.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How does the 'RegressionErrorNormality()' assess value normality?</td>\n",
       "      <td>RegressionErrorNormality() assesses value normality by visualizing the quantile-quantile plot (Q-Q plot).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the primary focus of the 'DiversityMetric' in recommendation systems?</td>\n",
       "      <td>The primary focus of the 'DiversityMetric' in recommendation systems is to measure the average intra-list diversity at K, reflecting the variety of items within the same user's recommendation list, averaged by all users.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How are missing values tested in the 'TestShareOfRowsWithMissingValues()'?</td>\n",
       "      <td>The 'TestShareOfRowsWithMissingValues()' tests the share of rows that contain missing values against a reference or a defined condition. With reference, the test fails if the share of rows with missing values is over 10% higher than in the reference. Without reference, the test fails if the dataset contains any rows with missing values.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How does the 'TestNumberOfDuplicatedRows()' function evaluate dataset integrity?\"</td>\n",
       "      <td>The 'TestNumberOfDuplicatedRows()' function evaluates dataset integrity by testing the number of duplicate rows against a reference or a defined condition. If a reference is provided, the test fails if the share of duplicate rows is over 10% higher or lower than in the reference. If no reference is provided, the test fails if there is at least one duplicate row.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                 Query  \\\n",
       "0                               \"How is the 'Name' of a Metric used in reading tables?   \n",
       "1   What information does the 'Description' section provide in the context of Metrics?   \n",
       "2                    What parameters are considered for the 'PopularityBias()' Metric?   \n",
       "3                      How does the 'TestFPR()' function operate at the dataset level?   \n",
       "4             What is the role of the 'TestGiniIndex(k=k)' in evaluating dataset bias?   \n",
       "5               What visualization is provided by the 'RegressionErrorDistribution()'?   \n",
       "6                    How does the 'RegressionErrorNormality()' assess value normality?   \n",
       "7        What is the primary focus of the 'DiversityMetric' in recommendation systems?   \n",
       "8           How are missing values tested in the 'TestShareOfRowsWithMissingValues()'?   \n",
       "9    How does the 'TestNumberOfDuplicatedRows()' function evaluate dataset integrity?\"   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                  Baseline_answers  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                 The 'Name' of a Metric is used to identify the specific Metric being referenced.  \n",
       "1                                                                                                                                                                                                                                                                                             The 'Description' section provides a plain text explanation of the Metric, specifying whether it applies to the whole dataset or individual columns.  \n",
       "2                                                                                                                                                                                                                                                                                                                                                   ARP, Coverage, and Gini index are the parameters considered for the 'PopularityBias()' Metric.  \n",
       "3                                                                                                                                                                                                                                                                                The `TestFPR()` function operates at the dataset level by computing the False Positive Rate (FPR) and comparing it to a reference or against a defined condition.  \n",
       "4  The role of the 'TestGiniIndex(k=k)' in evaluating dataset bias is to compute the Gini Index at the top K recommendations and compare it to a reference or a defined condition. If the Gini Index at the top K is over 10% higher or lower than the reference, the test fails. This helps in assessing the fairness and distribution of recommendations, indicating potential bias if the Gini Index significantly deviates from the reference.  \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                   Visualizes the distribution of the model error in a histogram.  \n",
       "6                                                                                                                                                                                                                                                                                                                                        RegressionErrorNormality() assesses value normality by visualizing the quantile-quantile plot (Q-Q plot).  \n",
       "7                                                                                                                                                                                                                     The primary focus of the 'DiversityMetric' in recommendation systems is to measure the average intra-list diversity at K, reflecting the variety of items within the same user's recommendation list, averaged by all users.  \n",
       "8                                                                                               The 'TestShareOfRowsWithMissingValues()' tests the share of rows that contain missing values against a reference or a defined condition. With reference, the test fails if the share of rows with missing values is over 10% higher than in the reference. Without reference, the test fails if the dataset contains any rows with missing values.  \n",
       "9                                                                     The 'TestNumberOfDuplicatedRows()' function evaluates dataset integrity by testing the number of duplicate rows against a reference or a defined condition. If a reference is provided, the test fails if the share of duplicate rows is over 10% higher or lower than in the reference. If no reference is provided, the test fails if there is at least one duplicate row.  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_dataset[[\"Query\", \"Baseline_answers\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8530a6c0-6d4d-4c44-be73-f7d3c7d88e50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
