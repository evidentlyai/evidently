{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a6e3449",
   "metadata": {},
   "source": [
    "# Prompt Optimization with Evidently: Tweet Generation Example\n",
    "This tutorial shows how to optimize prompts for generating engaging tweets using Evidently's `PromptOptimizer` API. \n",
    "We'll iteratively improve a tweet generation prompt to maximize how engaging LLM-generated tweets are, according to a classifier.\n",
    "\n",
    "## What you'll learn:\n",
    "- How to define a tweet generation function with OpenAI\n",
    "- How to set up an LLM judge to classify tweet engagement\n",
    "- How to optimize a tweet generation prompt based on feedback\n",
    "- How to inspect the best optimized prompt"
   ]
  },
  {
   "cell_type": "code",
   "id": "3c9fdb12",
   "metadata": {},
   "source": [
    "# Install packages if needed\n",
    "# !pip install evidently openai pandas"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3fcd60c7",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "\n",
    "from evidently.descriptors import LLMEval\n",
    "from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
    "from evidently.llm.optimization import PromptOptimizer, PromptExecutionLog, Params"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e62d754e",
   "metadata": {},
   "source": [
    "## Define a Tweet Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "id": "6103a03b",
   "metadata": {},
   "source": [
    "def basic_tweet_generation(topic, model=\"gpt-3.5-turbo\", instructions=\"\"):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": instructions},\n",
    "            {\"role\": \"user\", \"content\": f\"Write a short paragraph about {topic}\"}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4d4cf710",
   "metadata": {},
   "source": [
    "## Define a Tweet Quality Judge"
   ]
  },
  {
   "cell_type": "code",
   "id": "2d7f0b76",
   "metadata": {},
   "source": [
    "tweet_quality = BinaryClassificationPromptTemplate(\n",
    "    pre_messages=[(\"system\", \"You are evaluating the quality of tweets\")],\n",
    "    criteria=\"\"\"\n",
    "Text is ENGAGING if it meets at least one of the following:\n",
    "  • Strong hook (question, surprise, bold statement)\n",
    "  • Uses emotion, humor, or opinion\n",
    "  • Encourages interaction\n",
    "  • Shows personality or distinct tone\n",
    "  • Includes vivid language or emojis\n",
    "  • Sparks curiosity or insight\n",
    "\n",
    "Text is NEUTRAL if it lacks these qualities.\n",
    "\"\"\",\n",
    "    target_category=\"ENGAGING\",\n",
    "    non_target_category=\"NEUTRAL\",\n",
    "    uncertainty=\"non_target\",\n",
    "    include_reasoning=True,\n",
    ")\n",
    "\n",
    "judge = LLMEval(\"basic_tweet_generation.result\", template=tweet_quality,\n",
    "                provider=\"openai\", model=\"gpt-4o-mini\", alias=\"Tweet quality\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a9b49b50",
   "metadata": {},
   "source": [
    "## Define a Prompt Execution Function"
   ]
  },
  {
   "cell_type": "code",
   "id": "78245abf",
   "metadata": {},
   "source": [
    "def run_prompt(generation_prompt: str, context) -> pd.Series:\n",
    "    \"\"\"generate engaging tweets\"\"\"\n",
    "    my_topics = [\n",
    "        \"testing in AI engineering is as important as in development\",\n",
    "        \"CI/CD is applicable in AI\",\n",
    "        \"Collaboration of subject matter experts and AI engineers improves product\",\n",
    "        \"Start LLM apps development from test cases generation\",\n",
    "        \"evidently is a great tool for LLM testing\"\n",
    "    ]\n",
    "    tweets = [basic_tweet_generation(topic, model=\"gpt-3.5-turbo\", instructions=generation_prompt) for topic in my_topics * 3]\n",
    "    return pd.Series(tweets)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "53c2281d",
   "metadata": {},
   "source": [
    "## Run the Prompt Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "id": "6cbb4971",
   "metadata": {},
   "source": [
    "optimizer = PromptOptimizer(\"tweet_gen_example\", strategy=\"feedback\", verbose=True)\n",
    "await optimizer.arun(run_prompt, scorer=judge, base_prompt=\"You are tweet generator\", repetitions=5)\n",
    "# sync version\n",
    "# optimizer.run(run_prompt, scorer=judge)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ced639e2",
   "metadata": {},
   "source": [
    "## View the Best Optimized Prompt"
   ]
  },
  {
   "cell_type": "code",
   "id": "2cff870f",
   "metadata": {},
   "source": [
    "print(optimizer.best_prompt())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "optimizer.print_stats()",
   "id": "b865c5caa2f1cb4c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
