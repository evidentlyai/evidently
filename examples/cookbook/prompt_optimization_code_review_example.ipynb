{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15a7ac63",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ“Š Prompt Optimization with Evidently: Code Review Quality Classifier\n",
    "\n",
    "This tutorial demonstrates how to use Evidently's new `PromptOptimizer` API for optimizing prompts for LLM judges. \n",
    "We'll walk through optimizing a prompt that classifies the quality of code reviews written for junior developers.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… What you'll learn:\n",
    "- How to set up a dataset for LLM evaluation\n",
    "- How to define an LLM judge with a prompt template\n",
    "- How to run the prompt optimization loop\n",
    "- How to retrieve and inspect the best performing prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133c6d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you haven't installed the required packages yet:\n",
    "# !pip install evidently openai pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc9af2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from evidently import Dataset, DataDefinition, LLMClassification\n",
    "from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
    "from evidently.descriptors import LLMEval\n",
    "from evidently.llm.optimization import PromptOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5f6441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "review_dataset = pd.read_csv(\"../datasets/code_review.csv\")\n",
    "review_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e464810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how Evidently should interpret your dataset\n",
    "dd = DataDefinition(\n",
    "    text_columns=[\"Generated review\", \"Expert comment\"],\n",
    "    categorical_columns=[\"Expert label\"],\n",
    "    llm=LLMClassification(input=\"Generated review\", target=\"Expert label\", reasoning=\"Expert comment\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3957c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your pandas DataFrame into an Evidently Dataset\n",
    "dataset = Dataset.from_pandas(review_dataset, data_definition=dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af027bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt template and judge for classifying code review quality\n",
    "criteria = '''A review is GOOD when it's actionable and constructive.\n",
    "A review is BAD when it is non-actionable or overly critical.'''\n",
    "\n",
    "feedback_quality = BinaryClassificationPromptTemplate(\n",
    "    pre_messages=[(\"system\", \"You are evaluating the quality of code reviews given to junior developers.\")],\n",
    "    criteria=criteria,\n",
    "    target_category=\"bad\",\n",
    "    non_target_category=\"good\",\n",
    "    uncertainty=\"unknown\",\n",
    "    include_reasoning=True,\n",
    ")\n",
    "\n",
    "judge = LLMEval(\n",
    "    alias=\"Code Review Judge\",\n",
    "    provider=\"openai\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    column_name=\"Generated review\",\n",
    "    template=feedback_quality\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6995309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the optimizer and run optimization using feedback strategy\n",
    "optimizer = PromptOptimizer(\"code_review_example\", strategy=\"feedback\")\n",
    "optimizer.set_input_dataset(dataset)\n",
    "await optimizer.arun(judge, \"accuracy\")\n",
    "# for sync version:\n",
    "# optimizer.run(judge, \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f3162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the best-performing prompt template found by the optimizer\n",
    "print(optimizer.best_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aef48c-4369-4178-87ca-fd590eb2fad4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
