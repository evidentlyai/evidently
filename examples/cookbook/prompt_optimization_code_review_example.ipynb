{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15a7ac63",
   "metadata": {},
   "source": [
    "# Prompt Optimization with Evidently: Code Review Quality Classifier\n",
    "This tutorial demonstrates how to use Evidently's new `PromptOptimizer` API for optimizing prompts for LLM judges. \n",
    "We'll walk through optimizing a prompt that classifies the quality of code reviews written for junior developers.\n",
    "\n",
    "## What you'll learn:\n",
    "- How to set up a dataset for LLM evaluation\n",
    "- How to define an LLM judge with a prompt template\n",
    "- How to run the prompt optimization loop\n",
    "- How to retrieve and inspect the best performing prompt"
   ]
  },
  {
   "cell_type": "code",
   "id": "133c6d18",
   "metadata": {},
   "source": [
    "# If you haven't installed the required packages yet:\n",
    "# !pip install evidently openai pandas"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3cc9af2e",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "from evidently import Dataset, DataDefinition, LLMClassification\n",
    "from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
    "from evidently.descriptors import LLMEval\n",
    "from evidently.llm.optimization import PromptOptimizer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fd5f6441",
   "metadata": {},
   "source": [
    "# Load your dataset\n",
    "review_dataset = pd.read_csv(\"../datasets/code_review.csv\")\n",
    "review_dataset.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6e464810",
   "metadata": {},
   "source": [
    "# Define how Evidently should interpret your dataset\n",
    "dd = DataDefinition(\n",
    "    text_columns=[\"Generated review\", \"Expert comment\"],\n",
    "    categorical_columns=[\"Expert label\"],\n",
    "    llm=LLMClassification(input=\"Generated review\", target=\"Expert label\", reasoning=\"Expert comment\")\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3957c58d",
   "metadata": {},
   "source": [
    "# Convert your pandas DataFrame into an Evidently Dataset\n",
    "dataset = Dataset.from_pandas(review_dataset, data_definition=dd)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "af027bae",
   "metadata": {},
   "source": [
    "# Define a prompt template and judge for classifying code review quality\n",
    "criteria = '''A review is GOOD when it's actionable and constructive.\n",
    "A review is BAD when it is non-actionable or overly critical.'''\n",
    "\n",
    "feedback_quality = BinaryClassificationPromptTemplate(\n",
    "    pre_messages=[(\"system\", \"You are evaluating the quality of code reviews given to junior developers.\")],\n",
    "    criteria=criteria,\n",
    "    target_category=\"bad\",\n",
    "    non_target_category=\"good\",\n",
    "    uncertainty=\"unknown\",\n",
    "    include_reasoning=True,\n",
    ")\n",
    "\n",
    "judge = LLMEval(\n",
    "    alias=\"Code Review Judge\",\n",
    "    provider=\"openai\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    column_name=\"Generated review\",\n",
    "    template=feedback_quality\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6995309b",
   "metadata": {},
   "source": [
    "# Initialize the optimizer and run optimization using feedback strategy\n",
    "optimizer = PromptOptimizer(\"code_review_example\", strategy=\"feedback\", verbose=True)\n",
    "await optimizer.arun(executor=judge, scorer=\"accuracy\", dataset=dataset, repetitions=5)\n",
    "# for sync version:\n",
    "# optimizer.run(judge, \"accuracy\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e7f3162d",
   "metadata": {},
   "source": [
    "# Show the best-performing prompt template found by the optimizer\n",
    "print(optimizer.best_prompt())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# inspect optimizer statistics & logs\n",
    "optimizer.print_stats()"
   ],
   "id": "9abd1c4ea5585177",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### LLM Judge from scratch\n",
    "\n",
    "You can also create LLM judges from scratch if you have labeled dataset."
   ],
   "id": "19d88462d4f97af2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from evidently.llm.optimization import BlankLLMJudge\n",
    "\n",
    "optimizer = PromptOptimizer(\"code_review_scratch_example\", strategy=\"feedback\", verbose=True)\n",
    "await optimizer.arun(executor=BlankLLMJudge(), scorer=\"accuracy\", dataset=dataset, repetitions=5)"
   ],
   "id": "d78caae22b9749cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "new_judge = optimizer.best_executor()\n",
    "new_judge"
   ],
   "id": "8b168c087f71ddc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "optimizer.print_stats()",
   "id": "57f46604626cd00d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You can save and load optimized template for later use",
   "id": "d0eb0d104d0174d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from evidently.descriptors import LLMJudge\n",
    "from evidently.llm.templates import BaseLLMPromptTemplate\n",
    "\n",
    "new_judge.template.dump(\"my_template.yaml\")\n",
    "template = BaseLLMPromptTemplate.load(\"my_template.yaml\")\n",
    "my_judge = LLMJudge(provider=\"openai\", model=\"gpt-4o-mini\", template=template, input_column=\"Generated review\")"
   ],
   "id": "51dde31029349848",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "74dc06582a422643",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
