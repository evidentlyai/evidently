{
 "cells": [
  {
   "cell_type": "code",
   "id": "a923e24b",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from typing import Optional\n",
    "from typing import List\n",
    "\n",
    "from evidently import Dataset\n",
    "from evidently import DataDefinition\n",
    "from evidently import Recsys\n",
    "from evidently import Report\n",
    "\n",
    "from evidently.tests import lte, gte, lt, gt, is_in, not_in, eq, not_eq\n",
    "from evidently.tests import Reference\n",
    "\n",
    "# Import all recsys metrics\n",
    "from evidently.metrics import PrecisionTopK\n",
    "from evidently.metrics import RecallTopK\n",
    "from evidently.metrics import FBetaTopK\n",
    "from evidently.metrics import MAP\n",
    "from evidently.metrics import MRR\n",
    "from evidently.metrics import NDCG\n",
    "from evidently.metrics import HitRate\n",
    "from evidently.metrics import ScoreDistribution\n",
    "from evidently.metrics import Personalization\n",
    "from evidently.metrics import Diversity\n",
    "from evidently.metrics import RecCasesTable\n",
    "\n",
    "# Import recsys preset\n",
    "from evidently.presets import RecsysPreset\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "434ac67e",
   "metadata": {},
   "source": [
    "# Recommendation Systems Metrics\n",
    "\n",
    "This notebook demonstrates how to use Evidently's recommendation systems metrics to evaluate the performance of recommendation models.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Evidently provides comprehensive metrics for evaluating recommendation systems:\n",
    "\n",
    "### Top-K Metrics (DataframeValue)\n",
    "- **Precision@K**: Precision at different K values\n",
    "- **Recall@K**: Recall at different K values  \n",
    "- **F-Beta@K**: F-Beta score at different K values\n",
    "- **MAP@K**: Mean Average Precision at different K values\n",
    "- **MRR@K**: Mean Reciprocal Rank at different K values\n",
    "- **NDCG@K**: Normalized Discounted Cumulative Gain at different K values\n",
    "- **Hit Rate@K**: Hit rate at different K values\n",
    "\n",
    "### Additional Metrics\n",
    "- **Score Distribution**: Distribution of recommendation scores\n",
    "- **Personalization**: How diverse recommendations are across users\n",
    "- **Diversity**: How diverse recommendations are within each user's list\n",
    "- **Item Bias**: Analysis of item popularity bias\n",
    "- **User Bias**: Analysis of user-specific bias\n",
    "- **RecCasesTable**: Detailed recommendation cases for analysis\n",
    "\n",
    "### Preset\n",
    "- **RecsysPreset**: Complete set of recommendation metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a094d8f",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "Let's create sample recommendation data to demonstrate the metrics. We'll simulate a movie recommendation system with:\n",
    "- Users and items (movies)\n",
    "- Recommendation scores/predictions\n",
    "- User interactions (ratings)\n",
    "- Item features (genres)\n",
    "- User/item bias features\n",
    "\n",
    "**Note**: Since the new API doesn't support `additional_data` yet, we need to include the target ratings directly in the main dataset by merging recommendations with interactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "4004720a",
   "metadata": {},
   "source": [
    "# Create sample recommendation data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate users, items, and interactions\n",
    "n_users = 100\n",
    "n_items = 50\n",
    "n_interactions = 1000\n",
    "\n",
    "# User and item IDs\n",
    "user_ids = [f\"user_{i}\" for i in range(n_users)]\n",
    "item_ids = [f\"movie_{i}\" for i in range(n_items)]\n",
    "\n",
    "# Generate interactions (user-item pairs with ratings)\n",
    "interactions = []\n",
    "for _ in range(n_interactions):\n",
    "    user_id = np.random.choice(user_ids)\n",
    "    item_id = np.random.choice(item_ids)\n",
    "    rating = np.random.choice([1, 2, 3, 4, 5], p=[0.1, 0.1, 0.2, 0.3, 0.3])  # Higher ratings more likely\n",
    "    interactions.append({\n",
    "        'user_id': user_id,\n",
    "        'item_id': item_id,\n",
    "        'rating': rating\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "interactions_df = pd.DataFrame(interactions)\n",
    "\n",
    "# Generate recommendation scores (predictions)\n",
    "recommendations = []\n",
    "for user_id in user_ids:\n",
    "    # Each user gets recommendations for all items\n",
    "    for item_id in item_ids:\n",
    "        # Simulate recommendation score (higher = more likely to be recommended)\n",
    "        score = np.random.uniform(0, 1)\n",
    "        recommendations.append({\n",
    "            'user_id': user_id,\n",
    "            'item_id': item_id,\n",
    "            'prediction': score\n",
    "        })\n",
    "\n",
    "recommendations_df = pd.DataFrame(recommendations)\n",
    "\n",
    "print(f\"Interactions: {len(interactions_df)}\")\n",
    "print(f\"Recommendations: {len(recommendations_df)}\")\n",
    "print(f\"Users: {len(user_ids)}\")\n",
    "print(f\"Items: {len(item_ids)}\")\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nSample interactions:\")\n",
    "print(interactions_df.head())\n",
    "print(\"\\nSample recommendations:\")\n",
    "print(recommendations_df.head())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "58965f70",
   "metadata": {},
   "source": [
    "# Create item features (numerical) for diversity metrics\n",
    "# The Diversity metric expects numerical features for distance calculations\n",
    "# Let's create more structured features to get meaningful diversity values\n",
    "\n",
    "item_features = []\n",
    "for i, item_id in enumerate(item_ids):\n",
    "    # Create more structured features that will show diversity\n",
    "    # Feature 1: Genre cluster (create distinct clusters)\n",
    "    genre_cluster = i % 6  # 0-5 clusters, cycling through items\n",
    "    \n",
    "    # Feature 2: Year (group items by decades for more structure)\n",
    "    decade = 1990 + (i % 4) * 10  # 1990, 2000, 2010, 2020\n",
    "    \n",
    "    # Feature 3: Rating tier (create distinct rating groups)\n",
    "    rating_tier = 1.0 + (i % 5) * 0.8  # 1.0, 1.8, 2.6, 3.4, 4.2\n",
    "    \n",
    "    # Add some noise to make it more realistic\n",
    "    genre_cluster += np.random.normal(0, 0.1)\n",
    "    decade += np.random.randint(-2, 3)\n",
    "    rating_tier += np.random.normal(0, 0.1)\n",
    "    \n",
    "    item_features.append({\n",
    "        'item_id': item_id,\n",
    "        'genre_cluster': genre_cluster,\n",
    "        'release_decade': decade,\n",
    "        'rating_tier': rating_tier\n",
    "    })\n",
    "\n",
    "item_features_df = pd.DataFrame(item_features)\n",
    "\n",
    "# Create user/item bias features (keep these categorical for bias analysis)\n",
    "user_bias_features = []\n",
    "for user_id in user_ids:\n",
    "    age_group = np.random.choice(['18-25', '26-35', '36-45', '46-55', '55+'])\n",
    "    user_bias_features.append({\n",
    "        'user_id': user_id,\n",
    "        'age_group': age_group\n",
    "    })\n",
    "\n",
    "item_bias_features = []\n",
    "for item_id in item_ids:\n",
    "    popularity = np.random.choice(['Low', 'Medium', 'High'])\n",
    "    item_bias_features.append({\n",
    "        'item_id': item_id,\n",
    "        'popularity': popularity\n",
    "    })\n",
    "\n",
    "user_bias_df = pd.DataFrame(user_bias_features)\n",
    "item_bias_df = pd.DataFrame(item_bias_features)\n",
    "\n",
    "print(\"Item features sample (numerical for diversity):\")\n",
    "print(item_features_df.head())\n",
    "print(f\"\\nFeature ranges:\")\n",
    "print(f\"Genre cluster: {item_features_df['genre_cluster'].min():.2f} - {item_features_df['genre_cluster'].max():.2f}\")\n",
    "print(f\"Release decade: {item_features_df['release_decade'].min()} - {item_features_df['release_decade'].max()}\")\n",
    "print(f\"Rating tier: {item_features_df['rating_tier'].min():.2f} - {item_features_df['rating_tier'].max():.2f}\")\n",
    "print(\"\\nUser bias features sample:\")\n",
    "print(user_bias_df.head())\n",
    "print(\"\\nItem bias features sample:\")\n",
    "print(item_bias_df.head())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4807f878",
   "metadata": {},
   "source": [
    "# Create datasets for current and reference periods\n",
    "# Split interactions into current and reference periods\n",
    "split_point = len(interactions_df) // 2\n",
    "current_interactions = interactions_df.iloc[:split_point].copy()\n",
    "reference_interactions = interactions_df.iloc[split_point:].copy()\n",
    "\n",
    "# Add some temporal variation to recommendations\n",
    "current_recommendations = recommendations_df.copy()\n",
    "reference_recommendations = recommendations_df.copy()\n",
    "\n",
    "# Add some noise to reference recommendations to simulate model changes\n",
    "reference_recommendations['prediction'] = reference_recommendations['prediction'] + np.random.normal(0, 0.1, len(reference_recommendations))\n",
    "reference_recommendations['prediction'] = np.clip(reference_recommendations['prediction'], 0, 1)\n",
    "\n",
    "print(f\"Current interactions: {len(current_interactions)}\")\n",
    "print(f\"Reference interactions: {len(reference_interactions)}\")\n",
    "print(f\"Current recommendations: {len(current_recommendations)}\")\n",
    "print(f\"Reference recommendations: {len(reference_recommendations)}\")\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nCurrent interactions sample:\")\n",
    "print(current_interactions.head())\n",
    "print(\"\\nReference interactions sample:\")\n",
    "print(reference_interactions.head())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a9a65726",
   "metadata": {},
   "source": [
    "# Create datasets with ratings included in the main data\n",
    "# We need to merge recommendations with interactions to get the target ratings\n",
    "\n",
    "# Merge current recommendations with current interactions to get ratings\n",
    "current_data = current_recommendations.merge(\n",
    "    current_interactions[['user_id', 'item_id', 'rating']], \n",
    "    on=['user_id', 'item_id'], \n",
    "    how='left'\n",
    ")\n",
    "# Fill missing ratings with 0 (no interaction)\n",
    "current_data['rating'] = current_data['rating'].fillna(0)\n",
    "\n",
    "# Merge reference recommendations with reference interactions to get ratings\n",
    "reference_data = reference_recommendations.merge(\n",
    "    reference_interactions[['user_id', 'item_id', 'rating']], \n",
    "    on=['user_id', 'item_id'], \n",
    "    how='left'\n",
    ")\n",
    "# Fill missing ratings with 0 (no interaction)\n",
    "reference_data['rating'] = reference_data['rating'].fillna(0)\n",
    "\n",
    "# Create data definition\n",
    "data_definition = DataDefinition(\n",
    "    numerical_columns=[\"rating\", \"prediction\"],\n",
    "    categorical_columns=[\"user_id\", \"item_id\"],\n",
    "    ranking=[Recsys(\n",
    "        user_id=\"user_id\",\n",
    "        item_id=\"item_id\", \n",
    "        prediction=\"prediction\",\n",
    "        target=\"rating\"\n",
    "    )]\n",
    ")\n",
    "\n",
    "# Create current dataset\n",
    "current_dataset = Dataset.from_pandas(\n",
    "    current_data,\n",
    "    data_definition=data_definition\n",
    ")\n",
    "\n",
    "# Create reference dataset  \n",
    "reference_dataset = Dataset.from_pandas(\n",
    "    reference_data,\n",
    "    data_definition=data_definition\n",
    ")\n",
    "\n",
    "print(\"Datasets created successfully!\")\n",
    "print(f\"Current dataset shape: {current_dataset.as_dataframe().shape}\")\n",
    "print(f\"Reference dataset shape: {reference_dataset.as_dataframe().shape}\")\n",
    "print(f\"Current dataset with ratings: {current_data['rating'].notna().sum()} out of {len(current_data)}\")\n",
    "print(f\"Reference dataset with ratings: {reference_data['rating'].notna().sum()} out of {len(reference_data)}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "25b8d4b1",
   "metadata": {},
   "source": [
    "## Individual Metrics\n",
    "\n",
    "Let's explore each metric individually to understand their outputs and use cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707cc497",
   "metadata": {},
   "source": [
    "### Top-K Metrics\n",
    "\n",
    "These metrics return DataframeValue with rank and value columns, showing performance at different K values.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e721a0e6",
   "metadata": {},
   "source": [
    "# Precision@K - measures accuracy of recommendations\n",
    "precision_report = Report([\n",
    "    PrecisionTopK(k=10, min_rel_score=3)  # Consider ratings >= 3 as relevant\n",
    "])\n",
    "\n",
    "precision_snapshot = precision_report.run(current_dataset, reference_dataset)\n",
    "precision_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c629cb50",
   "metadata": {},
   "source": [
    "# Recall@K - measures coverage of relevant items\n",
    "recall_report = Report([\n",
    "    RecallTopK(k=10, min_rel_score=3)\n",
    "])\n",
    "\n",
    "recall_snapshot = recall_report.run(current_dataset, reference_dataset)\n",
    "recall_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b910076a",
   "metadata": {},
   "source": [
    "# F-Beta@K - harmonic mean of precision and recall\n",
    "fbeta_report = Report([\n",
    "    FBetaTopK(k=10, min_rel_score=3, beta=1.0)  # F1 score\n",
    "])\n",
    "\n",
    "fbeta_snapshot = fbeta_report.run(current_dataset, reference_dataset)\n",
    "fbeta_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "71a9f10f",
   "metadata": {},
   "source": [
    "# MAP@K - Mean Average Precision\n",
    "map_report = Report([\n",
    "    MAP(k=10, min_rel_score=3)\n",
    "])\n",
    "\n",
    "map_snapshot = map_report.run(current_dataset, reference_dataset)\n",
    "map_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "898dad41",
   "metadata": {},
   "source": [
    "# MRR@K - Mean Reciprocal Rank\n",
    "mrr_report = Report([\n",
    "    MRR(k=10, min_rel_score=3)\n",
    "])\n",
    "\n",
    "mrr_snapshot = mrr_report.run(current_dataset, reference_dataset)\n",
    "mrr_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7859b9b7",
   "metadata": {},
   "source": [
    "# NDCG@K - Normalized Discounted Cumulative Gain\n",
    "ndcg_report = Report([\n",
    "    NDCG(k=10, min_rel_score=3)\n",
    "])\n",
    "\n",
    "ndcg_snapshot = ndcg_report.run(current_dataset, reference_dataset)\n",
    "ndcg_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "73cebf5d",
   "metadata": {},
   "source": [
    "# Hit Rate@K - fraction of users with at least one relevant recommendation\n",
    "hitrate_report = Report([\n",
    "    HitRate(k=10, min_rel_score=3)\n",
    "])\n",
    "\n",
    "hitrate_snapshot = hitrate_report.run(current_dataset, reference_dataset)\n",
    "hitrate_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a2dce0bc",
   "metadata": {},
   "source": [
    "### Additional Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "408100e8",
   "metadata": {},
   "source": [
    "# Score Distribution - distribution of recommendation scores\n",
    "score_dist_report = Report([\n",
    "    ScoreDistribution(k=10)\n",
    "])\n",
    "\n",
    "score_dist_snapshot = score_dist_report.run(current_dataset, reference_dataset)\n",
    "score_dist_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cb4e4de2",
   "metadata": {},
   "source": [
    "# Personalization - how diverse recommendations are across users\n",
    "personalization_report = Report([\n",
    "    Personalization(k=10)\n",
    "])\n",
    "\n",
    "personalization_snapshot = personalization_report.run(current_dataset, reference_dataset)\n",
    "personalization_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "21959e90",
   "metadata": {},
   "source": [
    "# Diversity - how diverse recommendations are within each user's list\n",
    "# Note: This requires numerical item features for distance calculations\n",
    "# The Diversity metric uses cosine distance between item features\n",
    "\n",
    "# Create diversity data for both current and reference periods\n",
    "# Use the numerical item features directly (no need for mapping since they're already one-to-one)\n",
    "diversity_current_data = current_data.copy()  # Use current_data which already has ratings\n",
    "diversity_current_data = diversity_current_data.merge(item_features_df, on='item_id', how='left')\n",
    "\n",
    "diversity_reference_data = reference_data.copy()  # Use reference_data which already has ratings\n",
    "diversity_reference_data = diversity_reference_data.merge(item_features_df, on='item_id', how='left')\n",
    "\n",
    "# Fill missing numerical features with median values\n",
    "for col in ['genre_cluster', 'release_decade', 'rating_tier']:\n",
    "    median_val = diversity_current_data[col].median()\n",
    "    diversity_current_data[col] = diversity_current_data[col].fillna(median_val)\n",
    "    diversity_reference_data[col] = diversity_reference_data[col].fillna(median_val)\n",
    "\n",
    "# Create data definition with numerical item features\n",
    "diversity_data_definition = DataDefinition(\n",
    "    numerical_columns=[\"rating\", \"prediction\", \"genre_cluster\", \"release_decade\", \"rating_tier\"],\n",
    "    categorical_columns=[\"user_id\", \"item_id\"],\n",
    "    ranking=[Recsys(\n",
    "        user_id=\"user_id\",\n",
    "        item_id=\"item_id\", \n",
    "        prediction=\"prediction\",\n",
    "        target=\"rating\"\n",
    "    )]\n",
    ")\n",
    "\n",
    "# Create both current and reference datasets with numerical item features\n",
    "diversity_current_dataset = Dataset.from_pandas(\n",
    "    diversity_current_data,\n",
    "    data_definition=diversity_data_definition\n",
    ")\n",
    "\n",
    "diversity_reference_dataset = Dataset.from_pandas(\n",
    "    diversity_reference_data,\n",
    "    data_definition=diversity_data_definition\n",
    ")\n",
    "\n",
    "# Diversity metric with numerical features\n",
    "diversity_report = Report([\n",
    "    Diversity(k=10, item_features=[\"genre_cluster\", \"release_decade\", \"rating_tier\"])\n",
    "])\n",
    "\n",
    "diversity_snapshot = diversity_report.run(diversity_current_dataset, diversity_reference_dataset)\n",
    "diversity_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "119eef38",
   "metadata": {},
   "source": [
    "# RecCasesTable - detailed recommendation cases for analysis\n",
    "rec_cases_report = Report([\n",
    "    RecCasesTable(\n",
    "        user_ids=[\"user_0\", \"user_1\", \"user_2\"],  # Show cases for specific users\n",
    "        display_features=[\"genre_cluster\", \"release_decade\", \"rating_tier\"]  # Show numerical features\n",
    "    )\n",
    "])\n",
    "\n",
    "rec_cases_snapshot = rec_cases_report.run(diversity_current_dataset, diversity_reference_dataset)\n",
    "rec_cases_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4d84d502",
   "metadata": {},
   "source": [
    "## Using the RecsysPreset\n",
    "\n",
    "The RecsysPreset provides a comprehensive set of recommendation metrics in one go. It automatically includes all relevant metrics based on the available data.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8e8e9b4a",
   "metadata": {},
   "source": [
    "# Basic RecsysPreset with minimal configuration\n",
    "basic_preset_report = Report([\n",
    "    RecsysPreset(\n",
    "        k=10,\n",
    "        min_rel_score=3,\n",
    "        ranking_name=\"default\"\n",
    "    )\n",
    "])\n",
    "\n",
    "basic_preset_snapshot = basic_preset_report.run(current_dataset, reference_dataset)\n",
    "basic_preset_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "90105362",
   "metadata": {},
   "source": [
    "# Comprehensive RecsysPreset with all features\n",
    "comprehensive_preset_report = Report([\n",
    "    RecsysPreset(\n",
    "        k=10,\n",
    "        min_rel_score=3,\n",
    "        ranking_name=\"default\",\n",
    "        user_ids=[\"user_0\", \"user_1\", \"user_2\"],  # Specific users for RecCasesTable\n",
    "        display_features=[\"genre_cluster\", \"release_decade\", \"rating_tier\"],  # Features to display in RecCasesTable\n",
    "        item_features=[\"genre_cluster\", \"release_decade\", \"rating_tier\"],  # Item features for diversity metrics\n",
    "        item_bias_columns=[\"popularity\"],  # Item bias analysis\n",
    "        user_bias_columns=[\"age_group\"],  # User bias analysis\n",
    "        normalize_arp=True,  # Normalize ARP in popularity bias\n",
    "        beta=1.0  # Beta parameter for F-Beta score\n",
    "    )\n",
    "])\n",
    "\n",
    "comprehensive_preset_snapshot = comprehensive_preset_report.run(diversity_current_dataset, diversity_reference_dataset)\n",
    "comprehensive_preset_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e911bebb",
   "metadata": {},
   "source": [
    "## Metric Results Analysis\n",
    "\n",
    "Let's examine the structure of metric results to understand how to access the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ca5f5dc1",
   "metadata": {},
   "source": [
    "# Access metric results\n",
    "precision_result = precision_snapshot.context.get_metric_result(PrecisionTopK(k=10, min_rel_score=3))\n",
    "\n",
    "print(\"Precision@K Result Structure:\")\n",
    "print(f\"Type: {type(precision_result)}\")\n",
    "print(f\"Current value: {precision_result.value}\")\n",
    "try:\n",
    "    precision_reference = precision_snapshot.context.get_reference_metric_result(PrecisionTopK(k=10, min_rel_score=3).metric_id)\n",
    "    print(f\"Reference value: {precision_reference.value}\")\n",
    "except:\n",
    "    print(\"Reference value: None\")\n",
    "\n",
    "# Access the DataFrame values\n",
    "print(\"\\nCurrent Precision@K DataFrame:\")\n",
    "print(f\"Current value: {precision_result.value}\")\n",
    "    \n",
    "try:\n",
    "    precision_reference = precision_snapshot.context.get_reference_metric_result(PrecisionTopK(k=10, min_rel_score=3).metric_id)\n",
    "    print(f\"Reference value: {precision_reference.value}\")\n",
    "except:\n",
    "    print(\"Reference value: None\")\n",
    "    print(\"\\nReference Precision@K DataFrame:\")\n",
    "try:\n",
    "    precision_reference = precision_snapshot.context.get_reference_metric_result(PrecisionTopK(k=10, min_rel_score=3).metric_id)\n",
    "    print(f\"Reference value: {precision_reference.value}\")\n",
    "except:\n",
    "    print(\"Reference value: None\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f56d8efa",
   "metadata": {},
   "source": [
    "# Access single value results (like Personalization)\n",
    "personalization_result = personalization_snapshot.context.get_metric_result(Personalization(k=10))\n",
    "\n",
    "print(\"Personalization Result Structure:\")\n",
    "print(f\"Type: {type(personalization_result)}\")\n",
    "print(f\"Current value: {personalization_result.value}\")\n",
    "try:\n",
    "    personalization_reference = personalization_snapshot.context.get_reference_metric_result(Personalization(k=10).metric_id)\n",
    "    print(f\"Reference value: {personalization_reference.value}\")\n",
    "except:\n",
    "    print(\"Reference value: None\")\n",
    "\n",
    "# Access the single values\n",
    "print(f\"Current value: {personalization_result.value}\")\n",
    "print(f\"Current value: {personalization_result.value}\")\n",
    "    \n",
    "try:\n",
    "    personalization_reference = personalization_snapshot.context.get_reference_metric_result(Personalization(k=10).metric_id)\n",
    "    print(f\"Reference value: {personalization_reference.value}\")\n",
    "except:\n",
    "    print(\"Reference value: None\")\n",
    "try:\n",
    "    personalization_reference = personalization_snapshot.context.get_reference_metric_result(Personalization(k=10).metric_id)\n",
    "    print(f\"Reference value: {personalization_reference.value}\")\n",
    "except:\n",
    "    print(\"Reference value: None\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6968137a",
   "metadata": {},
   "source": [
    "# Access RecCasesTable results (DataframeValue with user_id column)\n",
    "rec_cases_result = rec_cases_snapshot.context.get_metric_result(RecCasesTable(user_ids=[\"user_0\", \"user_1\", \"user_2\"], display_features=[\"genre_cluster\", \"release_decade\", \"rating_tier\"]))\n",
    "\n",
    "print(\"RecCasesTable Result Structure:\")\n",
    "print(f\"Type: {type(rec_cases_result)}\")\n",
    "print(f\"Current value: {rec_cases_result.value}\")\n",
    "try:\n",
    "    rec_cases_reference = rec_cases_snapshot.context.get_reference_metric_result(RecCasesTable(user_ids=[\"user_0\", \"user_1\", \"user_2\"], display_features=[\"genre_cluster\", \"release_decade\", \"rating_tier\"]).metric_id)\n",
    "    print(f\"Reference value: {rec_cases_reference.value}\")\n",
    "except:\n",
    "    print(\"Reference value: None\")\n",
    "\n",
    "# Access the DataFrame values\n",
    "print(\"\\nCurrent RecCasesTable DataFrame:\")\n",
    "print(f\"Current value: {rec_cases_result.value}\")\n",
    "    \n",
    "try:\n",
    "    rec_cases_reference = rec_cases_snapshot.context.get_reference_metric_result(RecCasesTable(user_ids=[\"user_0\", \"user_1\", \"user_2\"], display_features=[\"genre_cluster\", \"release_decade\", \"rating_tier\"]).metric_id)\n",
    "    print(f\"Reference value: {rec_cases_reference.value}\")\n",
    "except:\n",
    "    print(\"Reference value: None\")\n",
    "    print(\"\\nReference RecCasesTable DataFrame:\")\n",
    "try:\n",
    "    rec_cases_reference = rec_cases_snapshot.context.get_reference_metric_result(RecCasesTable(user_ids=[\"user_0\", \"user_1\", \"user_2\"], display_features=[\"genre_cluster\", \"release_decade\", \"rating_tier\"]).metric_id)\n",
    "    print(f\"Reference value: {rec_cases_reference.value}\")\n",
    "except:\n",
    "    print(\"Reference value: None\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a3cc3233",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated how to use Evidently's recommendation systems metrics:\n",
    "\n",
    "### Key Features:\n",
    "1. **Top-K Metrics**: Return DataframeValue with rank and value columns showing performance at different K values\n",
    "2. **Single Value Metrics**: Return SingleValue for metrics like Personalization, Diversity, etc.\n",
    "3. **Bias Analysis**: Return DataframeValue with x, y columns for distribution analysis\n",
    "4. **RecCasesTable**: Return DataframeValue with user_id column for detailed case analysis\n",
    "\n",
    "### Data Requirements:\n",
    "- **Basic metrics**: User ID, Item ID, Prediction scores, Target ratings\n",
    "- **Diversity metrics**: Additional item features (e.g., genres)\n",
    "- **Bias metrics**: User/item bias features and training data\n",
    "- **RecCasesTable**: Optional user_ids and display_features\n",
    "\n",
    "### Preset Usage:\n",
    "- **RecsysPreset**: Automatically includes relevant metrics based on available data\n",
    "- **Conditional inclusion**: Metrics are added based on data availability (training data, features, etc.)\n",
    "\n",
    "### Result Access:\n",
    "- Use `.current` and `.reference` to access current and reference period results\n",
    "- Use `.value` to access the actual data (DataFrame for DataframeValue, number for SingleValue)\n",
    "- Results can be used for further analysis, visualization, or monitoring\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
