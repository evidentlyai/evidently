{
 "cells": [
  {
   "cell_type": "code",
   "id": "a923e24b",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from typing import Optional\n",
    "from typing import List\n",
    "\n",
    "from evidently import Dataset\n",
    "from evidently import DataDefinition\n",
    "from evidently import Recsys\n",
    "from evidently import Report\n",
    "\n",
    "from evidently.tests import lte, gte, lt, gt, is_in, not_in, eq, not_eq\n",
    "from evidently.tests import Reference\n",
    "\n",
    "# Import all recsys metrics\n",
    "from evidently.metrics import PrecisionTopK\n",
    "from evidently.metrics import RecallTopK\n",
    "from evidently.metrics import FBetaTopK\n",
    "from evidently.metrics import MAP\n",
    "from evidently.metrics import MRR\n",
    "from evidently.metrics import NDCG\n",
    "from evidently.metrics import HitRate\n",
    "from evidently.metrics import ScoreDistribution\n",
    "from evidently.metrics import Personalization\n",
    "from evidently.metrics import Diversity\n",
    "from evidently.metrics import RecCasesTable\n",
    "from evidently.metrics import ItemBias\n",
    "from evidently.metrics import UserBias\n",
    "from evidently.metrics import PopularityBiasMetric\n",
    "from evidently.metrics import Serendipity\n",
    "from evidently.metrics import Novelty\n",
    "\n",
    "# Import recsys preset\n",
    "from evidently.presets import RecsysPreset\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "434ac67e",
   "metadata": {},
   "source": [
    "# Recommendation Systems Metrics\n",
    "\n",
    "This notebook demonstrates how to use Evidently's recommendation systems metrics to evaluate the performance of recommendation models.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Evidently provides comprehensive metrics for evaluating recommendation systems:\n",
    "\n",
    "### Top-K Metrics (DataframeValue)\n",
    "- **Precision@K**: Precision at different K values\n",
    "- **Recall@K**: Recall at different K values  \n",
    "- **F-Beta@K**: F-Beta score at different K values\n",
    "- **MAP@K**: Mean Average Precision at different K values\n",
    "- **MRR@K**: Mean Reciprocal Rank at different K values\n",
    "- **NDCG@K**: Normalized Discounted Cumulative Gain at different K values\n",
    "- **Hit Rate@K**: Hit rate at different K values\n",
    "\n",
    "### Additional Metrics\n",
    "- **Score Distribution**: Distribution of recommendation scores\n",
    "- **Personalization**: How diverse recommendations are across users\n",
    "- **Diversity**: How diverse recommendations are within each user's list\n",
    "- **Item Bias**: Analysis of item popularity bias\n",
    "- **User Bias**: Analysis of user-specific bias\n",
    "- **RecCasesTable**: Detailed recommendation cases for analysis\n",
    "\n",
    "### Preset\n",
    "- **RecsysPreset**: Complete set of recommendation metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a094d8f",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "Let's create sample recommendation data to demonstrate the metrics. We'll simulate a movie recommendation system with:\n",
    "- Users and items (movies)\n",
    "- Recommendation scores/predictions\n",
    "- User interactions (ratings)\n",
    "- Item features (genres)\n",
    "- User/item bias features\n",
    "\n",
    "**Note**: Since the new API doesn't support `additional_data` yet, we need to include the target ratings directly in the main dataset by merging recommendations with interactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "4004720a",
   "metadata": {},
   "source": [
    "# Create sample recommendation data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate users, items, and interactions\n",
    "n_users = 100\n",
    "n_items = 50\n",
    "n_interactions = 1000\n",
    "\n",
    "# User and item IDs\n",
    "user_ids = [f\"user_{i}\" for i in range(n_users)]\n",
    "item_ids = [f\"movie_{i}\" for i in range(n_items)]\n",
    "\n",
    "# Generate interactions (user-item pairs with ratings)\n",
    "interactions = []\n",
    "for _ in range(n_interactions):\n",
    "    user_id = np.random.choice(user_ids)\n",
    "    item_id = np.random.choice(item_ids)\n",
    "    rating = np.random.choice([1, 2, 3, 4, 5], p=[0.1, 0.1, 0.2, 0.3, 0.3])  # Higher ratings more likely\n",
    "    interactions.append({\n",
    "        'user_id': user_id,\n",
    "        'item_id': item_id,\n",
    "        'rating': rating\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "interactions_df = pd.DataFrame(interactions)\n",
    "\n",
    "# Generate recommendation scores (predictions)\n",
    "recommendations = []\n",
    "for user_id in user_ids:\n",
    "    # Each user gets recommendations for all items\n",
    "    for item_id in item_ids:\n",
    "        # Simulate recommendation score (higher = more likely to be recommended)\n",
    "        score = np.random.uniform(0, 1)\n",
    "        recommendations.append({\n",
    "            'user_id': user_id,\n",
    "            'item_id': item_id,\n",
    "            'prediction': score\n",
    "        })\n",
    "\n",
    "recommendations_df = pd.DataFrame(recommendations)\n",
    "\n",
    "print(f\"Interactions: {len(interactions_df)}\")\n",
    "print(f\"Recommendations: {len(recommendations_df)}\")\n",
    "print(f\"Users: {len(user_ids)}\")\n",
    "print(f\"Items: {len(item_ids)}\")\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nSample interactions:\")\n",
    "print(interactions_df.head())\n",
    "print(\"\\nSample recommendations:\")\n",
    "print(recommendations_df.head())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "58965f70",
   "metadata": {},
   "source": [
    "# Create item features (numerical) for diversity metrics\n",
    "# The Diversity metric expects numerical features for distance calculations\n",
    "# Let's create more structured features to get meaningful diversity values\n",
    "\n",
    "item_features = []\n",
    "for i, item_id in enumerate(item_ids):\n",
    "    # Create more structured features that will show diversity\n",
    "    # Feature 1: Genre cluster (create distinct clusters)\n",
    "    genre_cluster = i % 6  # 0-5 clusters, cycling through items\n",
    "    \n",
    "    # Feature 2: Year (group items by decades for more structure)\n",
    "    decade = (1990 + (i % 4) * 10) % 100  # (19)90, (20)00, (20)10, (20)20\n",
    "    \n",
    "    # Feature 3: Rating tier (create distinct rating groups)\n",
    "    rating_tier = 1.0 + (i % 5) * 0.8  # 1.0, 1.8, 2.6, 3.4, 4.2\n",
    "    \n",
    "    # Add some noise to make it more realistic\n",
    "    genre_cluster += np.random.normal(0, 0.1)\n",
    "    decade += np.random.randint(-2, 3)\n",
    "    rating_tier += np.random.normal(0, 0.1)\n",
    "    \n",
    "    item_features.append({\n",
    "        'item_id': item_id,\n",
    "        'genre_cluster': genre_cluster,\n",
    "        'release_decade': decade,\n",
    "        'rating_tier': rating_tier\n",
    "    })\n",
    "\n",
    "item_features_df = pd.DataFrame(item_features)\n",
    "\n",
    "# Create user/item bias features (keep these categorical for bias analysis)\n",
    "user_bias_features = []\n",
    "for user_id in user_ids:\n",
    "    age_group = np.random.choice(['18-25', '26-35', '36-45', '46-55', '55+'])\n",
    "    user_bias_features.append({\n",
    "        'user_id': user_id,\n",
    "        'age_group': age_group\n",
    "    })\n",
    "\n",
    "item_bias_features = []\n",
    "for item_id in item_ids:\n",
    "    popularity = np.random.choice(['Low', 'Medium', 'High'])\n",
    "    item_bias_features.append({\n",
    "        'item_id': item_id,\n",
    "        'popularity': popularity\n",
    "    })\n",
    "\n",
    "user_bias_df = pd.DataFrame(user_bias_features)\n",
    "item_bias_df = pd.DataFrame(item_bias_features)\n",
    "\n",
    "print(\"Item features sample (numerical for diversity):\")\n",
    "print(item_features_df.head())\n",
    "print(f\"\\nFeature ranges:\")\n",
    "print(f\"Genre cluster: {item_features_df['genre_cluster'].min():.2f} - {item_features_df['genre_cluster'].max():.2f}\")\n",
    "print(f\"Release decade: {item_features_df['release_decade'].min()} - {item_features_df['release_decade'].max()}\")\n",
    "print(f\"Rating tier: {item_features_df['rating_tier'].min():.2f} - {item_features_df['rating_tier'].max():.2f}\")\n",
    "print(\"\\nUser bias features sample:\")\n",
    "print(user_bias_df.head())\n",
    "print(\"\\nItem bias features sample:\")\n",
    "print(item_bias_df.head())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4807f878",
   "metadata": {},
   "source": [
    "# Create datasets for current and reference periods\n",
    "# Split interactions into current and reference periods\n",
    "split_point = len(interactions_df) // 2\n",
    "current_interactions = interactions_df.iloc[:split_point].copy()\n",
    "reference_interactions = interactions_df.iloc[split_point:].copy()\n",
    "\n",
    "# Add some temporal variation to recommendations\n",
    "current_recommendations = recommendations_df.copy()\n",
    "reference_recommendations = recommendations_df.copy()\n",
    "\n",
    "# Add some noise to reference recommendations to simulate model changes\n",
    "reference_recommendations['prediction'] = reference_recommendations['prediction'] + np.random.normal(0, 0.1, len(reference_recommendations))\n",
    "reference_recommendations['prediction'] = np.clip(reference_recommendations['prediction'], 0, 1)\n",
    "\n",
    "print(f\"Current interactions: {len(current_interactions)}\")\n",
    "print(f\"Reference interactions: {len(reference_interactions)}\")\n",
    "print(f\"Current recommendations: {len(current_recommendations)}\")\n",
    "print(f\"Reference recommendations: {len(reference_recommendations)}\")\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nCurrent interactions sample:\")\n",
    "print(current_interactions.head())\n",
    "print(\"\\nReference interactions sample:\")\n",
    "print(reference_interactions.head())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a9a65726",
   "metadata": {},
   "source": [
    "# Create datasets with ratings included in the main data\n",
    "# We need to merge recommendations with interactions to get the target ratings\n",
    "\n",
    "# Merge current recommendations with current interactions to get ratings\n",
    "current_data = current_recommendations.merge(\n",
    "    current_interactions[['user_id', 'item_id', 'rating']], \n",
    "    on=['user_id', 'item_id'], \n",
    "    how='left'\n",
    ")\n",
    "# Fill missing ratings with 0 (no interaction)\n",
    "current_data['rating'] = current_data['rating'].fillna(0)\n",
    "\n",
    "# Merge reference recommendations with reference interactions to get ratings\n",
    "reference_data = reference_recommendations.merge(\n",
    "    reference_interactions[['user_id', 'item_id', 'rating']], \n",
    "    on=['user_id', 'item_id'], \n",
    "    how='left'\n",
    ")\n",
    "# Fill missing ratings with 0 (no interaction)\n",
    "reference_data['rating'] = reference_data['rating'].fillna(0)\n",
    "\n",
    "# Create data definition\n",
    "data_definition = DataDefinition(\n",
    "    numerical_columns=[\"rating\", \"prediction\"],\n",
    "    categorical_columns=[\"user_id\", \"item_id\"],\n",
    "    ranking=[Recsys(\n",
    "        user_id=\"user_id\",\n",
    "        item_id=\"item_id\", \n",
    "        prediction=\"prediction\",\n",
    "        target=\"rating\"\n",
    "    )]\n",
    ")\n",
    "\n",
    "# Create current dataset\n",
    "current_dataset = Dataset.from_pandas(\n",
    "    current_data,\n",
    "    data_definition=data_definition\n",
    ")\n",
    "\n",
    "# Create reference dataset  \n",
    "reference_dataset = Dataset.from_pandas(\n",
    "    reference_data,\n",
    "    data_definition=data_definition\n",
    ")\n",
    "\n",
    "print(\"Datasets created successfully!\")\n",
    "print(f\"Current dataset shape: {current_dataset.as_dataframe().shape}\")\n",
    "print(f\"Reference dataset shape: {reference_dataset.as_dataframe().shape}\")\n",
    "print(f\"Current dataset with ratings: {current_data['rating'].notna().sum()} out of {len(current_data)}\")\n",
    "print(f\"Reference dataset with ratings: {reference_data['rating'].notna().sum()} out of {len(reference_data)}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "25b8d4b1",
   "metadata": {},
   "source": [
    "## Individual Metrics\n",
    "\n",
    "Let's explore each metric individually to understand their outputs and use cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707cc497",
   "metadata": {},
   "source": [
    "### Top-K Metrics\n",
    "\n",
    "These metrics return DataframeValue with rank and value columns, showing performance at different K values.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e721a0e6",
   "metadata": {},
   "source": [
    "# Precision@K - measures accuracy of recommendations\n",
    "precision_report = Report([\n",
    "    PrecisionTopK(k=10, min_rel_score=3)  # Consider ratings >= 3 as relevant\n",
    "])\n",
    "\n",
    "precision_snapshot = precision_report.run(current_dataset, reference_dataset)\n",
    "precision_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c629cb50",
   "metadata": {},
   "source": [
    "# Recall@K - measures coverage of relevant items\n",
    "recall_report = Report([\n",
    "    RecallTopK(k=10, min_rel_score=3)\n",
    "])\n",
    "\n",
    "recall_snapshot = recall_report.run(current_dataset, reference_dataset)\n",
    "recall_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b910076a",
   "metadata": {},
   "source": [
    "# F-Beta@K - harmonic mean of precision and recall\n",
    "fbeta_report = Report([\n",
    "    FBetaTopK(k=10, min_rel_score=3, beta=1.0)  # F1 score\n",
    "])\n",
    "\n",
    "fbeta_snapshot = fbeta_report.run(current_dataset, reference_dataset)\n",
    "fbeta_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "71a9f10f",
   "metadata": {},
   "source": [
    "# MAP@K - Mean Average Precision\n",
    "map_report = Report([\n",
    "    MAP(k=10, min_rel_score=3)\n",
    "])\n",
    "\n",
    "map_snapshot = map_report.run(current_dataset, reference_dataset)\n",
    "map_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "898dad41",
   "metadata": {},
   "source": [
    "# MRR@K - Mean Reciprocal Rank\n",
    "mrr_report = Report([\n",
    "    MRR(k=10, min_rel_score=3)\n",
    "])\n",
    "\n",
    "mrr_snapshot = mrr_report.run(current_dataset, reference_dataset)\n",
    "mrr_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7859b9b7",
   "metadata": {},
   "source": [
    "# NDCG@K - Normalized Discounted Cumulative Gain\n",
    "ndcg_report = Report([\n",
    "    NDCG(k=10, min_rel_score=3)\n",
    "])\n",
    "\n",
    "ndcg_snapshot = ndcg_report.run(current_dataset, reference_dataset)\n",
    "ndcg_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "73cebf5d",
   "metadata": {},
   "source": [
    "# Hit Rate@K - fraction of users with at least one relevant recommendation\n",
    "hitrate_report = Report([\n",
    "    HitRate(k=10, min_rel_score=3)\n",
    "])\n",
    "\n",
    "hitrate_snapshot = hitrate_report.run(current_dataset, reference_dataset)\n",
    "hitrate_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a2dce0bc",
   "metadata": {},
   "source": [
    "### Additional Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "408100e8",
   "metadata": {},
   "source": [
    "# Score Distribution - distribution of recommendation scores\n",
    "score_dist_report = Report([\n",
    "    ScoreDistribution(k=10)\n",
    "])\n",
    "\n",
    "score_dist_snapshot = score_dist_report.run(current_dataset, reference_dataset)\n",
    "score_dist_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cb4e4de2",
   "metadata": {},
   "source": [
    "# Personalization - how diverse recommendations are across users\n",
    "personalization_report = Report([\n",
    "    Personalization(k=10)\n",
    "])\n",
    "\n",
    "personalization_snapshot = personalization_report.run(current_dataset, reference_dataset)\n",
    "personalization_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "21959e90",
   "metadata": {},
   "source": [
    "# Diversity - how diverse recommendations are within each user's list\n",
    "# Note: This requires numerical item features for distance calculations\n",
    "# The Diversity metric uses cosine distance between item features\n",
    "\n",
    "# Create diversity data for both current and reference periods\n",
    "# Use the numerical item features directly (no need for mapping since they're already one-to-one)\n",
    "diversity_current_data = current_data.copy()  # Use current_data which already has ratings\n",
    "diversity_current_data = diversity_current_data.merge(item_features_df, on='item_id', how='left')\n",
    "\n",
    "diversity_reference_data = reference_data.copy()  # Use reference_data which already has ratings\n",
    "diversity_reference_data = diversity_reference_data.merge(item_features_df, on='item_id', how='left')\n",
    "\n",
    "# Fill missing numerical features with median values\n",
    "for col in ['genre_cluster', 'release_decade', 'rating_tier']:\n",
    "    median_val = diversity_current_data[col].median()\n",
    "    diversity_current_data[col] = diversity_current_data[col].fillna(median_val)\n",
    "    diversity_reference_data[col] = diversity_reference_data[col].fillna(median_val)\n",
    "\n",
    "# Create data definition with numerical item features\n",
    "diversity_data_definition = DataDefinition(\n",
    "    numerical_columns=[\"rating\", \"prediction\", \"genre_cluster\", \"release_decade\", \"rating_tier\"],\n",
    "    categorical_columns=[\"user_id\", \"item_id\"],\n",
    "    ranking=[Recsys(\n",
    "        user_id=\"user_id\",\n",
    "        item_id=\"item_id\", \n",
    "        prediction=\"prediction\",\n",
    "        target=\"rating\"\n",
    "    )]\n",
    ")\n",
    "\n",
    "# Create both current and reference datasets with numerical item features\n",
    "diversity_current_dataset = Dataset.from_pandas(\n",
    "    diversity_current_data,\n",
    "    data_definition=diversity_data_definition\n",
    ")\n",
    "\n",
    "diversity_reference_dataset = Dataset.from_pandas(\n",
    "    diversity_reference_data,\n",
    "    data_definition=diversity_data_definition\n",
    ")\n",
    "\n",
    "# Diversity metric with numerical features\n",
    "diversity_report = Report([\n",
    "    Diversity(k=10, item_features=[\"genre_cluster\", \"release_decade\", \"rating_tier\"])\n",
    "])\n",
    "\n",
    "diversity_snapshot = diversity_report.run(diversity_current_dataset, diversity_reference_dataset)\n",
    "diversity_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "119eef38",
   "metadata": {},
   "source": [
    "# RecCasesTable - detailed recommendation cases for analysis\n",
    "rec_cases_report = Report([\n",
    "    RecCasesTable(\n",
    "        user_ids=[\"user_0\", \"user_1\", \"user_2\"],  # Show cases for specific users\n",
    "        display_features=[\"genre_cluster\", \"release_decade\", \"rating_tier\"]  # Show numerical features\n",
    "    )\n",
    "])\n",
    "\n",
    "rec_cases_snapshot = rec_cases_report.run(diversity_current_dataset, diversity_reference_dataset)\n",
    "rec_cases_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "additional_metrics_header",
   "metadata": {},
   "source": [
    "## Additional Recommendation Metrics\n",
    "\n",
    "The following metrics provide additional insights into recommendation system performance:\n",
    "\n",
    "### Bias Metrics (DataframeValue)\n",
    "- **ItemBias**: Analyzes bias in item recommendations\n",
    "- **UserBias**: Analyzes bias in user recommendations  \n",
    "- **PopularityBiasMetric**: Measures popularity bias in recommendations\n",
    "\n",
    "### Novelty and Serendipity Metrics (SingleValue)\n",
    "- **Novelty**: Measures how novel the recommendations are\n",
    "- **Serendipity**: Measures how surprising the recommendations are\n",
    "\n",
    "These metrics help identify potential biases and assess the diversity and surprise factor of recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "item_bias_example",
   "metadata": {},
   "source": [
    "# ItemBias - analyzes bias in item recommendations\n",
    "# Note: This requires training data and item bias columns\n",
    "# The training data should contain historical item features, not current recommendations\n",
    "\n",
    "# Create training data with item features (this represents historical data used to train the model)\n",
    "item_bias_train_data = item_bias_df.copy()  # Use the item features as training data\n",
    "item_bias_train_data['user_id'] = 'train_user'  # Add dummy user_id for training data\n",
    "item_bias_train_data['item_id'] = item_bias_train_data.index  # Use index as item_id\n",
    "item_bias_train_data['prediction'] = 0.5  # Dummy prediction for training data\n",
    "item_bias_train_data['rating'] = 3.0  # Dummy rating for training data\n",
    "\n",
    "# Convert categorical popularity to numeric (Low=1, Medium=2, High=3)\n",
    "popularity_mapping = {'Low': 1, 'Medium': 2, 'High': 3}\n",
    "item_bias_train_data['popularity'] = item_bias_train_data['popularity'].map(popularity_mapping)\n",
    "item_bias_train_data['popularity'] = item_bias_train_data['popularity'].fillna(item_bias_train_data['popularity'].median())\n",
    "\n",
    "# Create training data definition\n",
    "item_bias_train_data_definition = DataDefinition(\n",
    "    numerical_columns=[\"rating\", \"prediction\", \"popularity\"],\n",
    "    categorical_columns=[\"user_id\", \"item_id\"],\n",
    "    ranking=[Recsys(\n",
    "        user_id=\"user_id\",\n",
    "        item_id=\"item_id\", \n",
    "        prediction=\"prediction\",\n",
    "        target=\"rating\"\n",
    "    )]\n",
    ")\n",
    "\n",
    "# Create training dataset\n",
    "item_bias_train_dataset = Dataset.from_pandas(\n",
    "    item_bias_train_data,\n",
    "    data_definition=item_bias_train_data_definition\n",
    ")\n",
    "\n",
    "# Create current data with item features for recommendations\n",
    "item_bias_current_data = current_data.copy()\n",
    "item_bias_current_data = item_bias_current_data.merge(item_bias_df, on='item_id', how='left')\n",
    "\n",
    "item_bias_reference_data = reference_data.copy()\n",
    "item_bias_reference_data = item_bias_reference_data.merge(item_bias_df, on='item_id', how='left')\n",
    "\n",
    "# Convert categorical popularity to numeric for current data\n",
    "item_bias_current_data['popularity'] = item_bias_current_data['popularity'].map(popularity_mapping)\n",
    "item_bias_reference_data['popularity'] = item_bias_reference_data['popularity'].map(popularity_mapping)\n",
    "\n",
    "# Fill missing popularity values with median\n",
    "item_bias_current_data['popularity'] = item_bias_current_data['popularity'].fillna(item_bias_current_data['popularity'].median())\n",
    "item_bias_reference_data['popularity'] = item_bias_reference_data['popularity'].fillna(item_bias_reference_data['popularity'].median())\n",
    "\n",
    "# Create data definition with item bias columns\n",
    "item_bias_data_definition = DataDefinition(\n",
    "    numerical_columns=[\"rating\", \"prediction\", \"popularity\"],\n",
    "    categorical_columns=[\"user_id\", \"item_id\"],\n",
    "    ranking=[Recsys(\n",
    "        user_id=\"user_id\",\n",
    "        item_id=\"item_id\", \n",
    "        prediction=\"prediction\",\n",
    "        target=\"rating\"\n",
    "    )]\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "item_bias_current_dataset = Dataset.from_pandas(\n",
    "    item_bias_current_data,\n",
    "    data_definition=item_bias_data_definition\n",
    ")\n",
    "\n",
    "item_bias_reference_dataset = Dataset.from_pandas(\n",
    "    item_bias_reference_data,\n",
    "    data_definition=item_bias_data_definition\n",
    ")\n",
    "\n",
    "# ItemBias metric - note: uses column_name, not item_bias_columns\n",
    "item_bias_report = Report([\n",
    "    ItemBias(k=10, column_name=\"popularity\", distribution=\"default\")\n",
    "])\n",
    "\n",
    "item_bias_snapshot = item_bias_report.run(\n",
    "    item_bias_current_dataset, \n",
    "    item_bias_reference_dataset,\n",
    "    additional_data={\n",
    "        \"current_train_data\": item_bias_train_dataset,  # Historical training data\n",
    "        \"reference_train_data\": item_bias_train_dataset  # Same training data for reference\n",
    "    }\n",
    ")\n",
    "item_bias_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "user_bias_example",
   "metadata": {},
   "source": [
    "# UserBias - analyzes bias in user recommendations\n",
    "# Note: This requires training data and user bias columns\n",
    "# The training data should contain historical user features, not current recommendations\n",
    "\n",
    "# Create training data with user features (this represents historical data used to train the model)\n",
    "user_bias_train_data = user_bias_df.copy()  # Use the user features as training data\n",
    "user_bias_train_data['item_id'] = 'train_item'  # Add dummy item_id for training data\n",
    "user_bias_train_data['user_id'] = user_bias_train_data.index  # Use index as user_id\n",
    "user_bias_train_data['prediction'] = 0.5  # Dummy prediction for training data\n",
    "user_bias_train_data['rating'] = 3.0  # Dummy rating for training data\n",
    "\n",
    "# Fill missing age_group values\n",
    "user_bias_train_data['age_group'] = user_bias_train_data['age_group'].fillna('Unknown')\n",
    "\n",
    "# Create training data definition\n",
    "user_bias_train_data_definition = DataDefinition(\n",
    "    numerical_columns=[\"rating\", \"prediction\"],\n",
    "    categorical_columns=[\"user_id\", \"item_id\", \"age_group\"],\n",
    "    ranking=[Recsys(\n",
    "        user_id=\"user_id\",\n",
    "        item_id=\"item_id\", \n",
    "        prediction=\"prediction\",\n",
    "        target=\"rating\"\n",
    "    )]\n",
    ")\n",
    "\n",
    "# Create training dataset\n",
    "user_bias_train_dataset = Dataset.from_pandas(\n",
    "    user_bias_train_data,\n",
    "    data_definition=user_bias_train_data_definition\n",
    ")\n",
    "\n",
    "# Create current data with user features for recommendations\n",
    "user_bias_current_data = current_data.copy()\n",
    "user_bias_current_data = user_bias_current_data.merge(user_bias_df, on='user_id', how='left')\n",
    "\n",
    "user_bias_reference_data = reference_data.copy()\n",
    "user_bias_reference_data = user_bias_reference_data.merge(user_bias_df, on='user_id', how='left')\n",
    "\n",
    "# Fill missing age_group values\n",
    "user_bias_current_data['age_group'] = user_bias_current_data['age_group'].fillna('Unknown')\n",
    "user_bias_reference_data['age_group'] = user_bias_reference_data['age_group'].fillna('Unknown')\n",
    "\n",
    "# Create data definition with user bias columns\n",
    "user_bias_data_definition = DataDefinition(\n",
    "    numerical_columns=[\"rating\", \"prediction\"],\n",
    "    categorical_columns=[\"user_id\", \"item_id\", \"age_group\"],\n",
    "    ranking=[Recsys(\n",
    "        user_id=\"user_id\",\n",
    "        item_id=\"item_id\", \n",
    "        prediction=\"prediction\",\n",
    "        target=\"rating\"\n",
    "    )]\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "user_bias_current_dataset = Dataset.from_pandas(\n",
    "    user_bias_current_data,\n",
    "    data_definition=user_bias_data_definition\n",
    ")\n",
    "\n",
    "user_bias_reference_dataset = Dataset.from_pandas(\n",
    "    user_bias_reference_data,\n",
    "    data_definition=user_bias_data_definition\n",
    ")\n",
    "\n",
    "# UserBias metric - note: uses column_name, not user_bias_columns\n",
    "user_bias_report = Report([\n",
    "    UserBias(column_name=\"age_group\", distribution=\"default\")\n",
    "])\n",
    "\n",
    "user_bias_snapshot = user_bias_report.run(\n",
    "    user_bias_current_dataset, \n",
    "    user_bias_reference_dataset,\n",
    "    additional_data={\n",
    "        \"current_train_data\": user_bias_train_dataset,  # Historical training data\n",
    "        \"reference_train_data\": user_bias_train_dataset  # Same training data for reference\n",
    "    }\n",
    ")\n",
    "user_bias_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "popularity_bias_example",
   "metadata": {},
   "source": [
    "# PopularityBiasMetric - measures popularity bias in recommendations\n",
    "# Create more realistic popularity distribution for better metrics\n",
    "import numpy as np\n",
    "\n",
    "# Create training data with realistic popularity distribution\n",
    "# Some items are very popular, others are niche\n",
    "np.random.seed(42)  # For reproducibility\n",
    "n_items = len(current_data['item_id'].unique())\n",
    "n_users = len(current_data['user_id'].unique())\n",
    "\n",
    "# Create popularity distribution (some items much more popular than others)\n",
    "item_popularity = {}\n",
    "for i, item_id in enumerate(current_data['item_id'].unique()):\n",
    "    # Create power-law distribution: some items very popular, others rare\n",
    "    popularity = np.random.pareto(1.5) + 1  # Pareto distribution for realistic popularity\n",
    "    item_popularity[item_id] = popularity\n",
    "\n",
    "# Create training data with this popularity distribution\n",
    "popularity_train_data = []\n",
    "for user_id in current_data['user_id'].unique():\n",
    "    # Each user interacts with different number of items (some users more active)\n",
    "    n_interactions = np.random.poisson(5) + 1  # Average 6 interactions per user\n",
    "    user_items = np.random.choice(\n",
    "        list(item_popularity.keys()), \n",
    "        size=min(n_interactions, n_items), \n",
    "        replace=False\n",
    "    )\n",
    "    for item_id in user_items:\n",
    "        popularity_train_data.append({\n",
    "            'user_id': user_id,\n",
    "            'item_id': item_id,\n",
    "            'prediction': 0.5,  # Dummy prediction\n",
    "            'rating': 3.0,  # Dummy rating\n",
    "            'popularity': item_popularity[item_id]\n",
    "        })\n",
    "\n",
    "popularity_train_df = pd.DataFrame(popularity_train_data)\n",
    "\n",
    "# Create training dataset\n",
    "popularity_train_data_definition = DataDefinition(\n",
    "    numerical_columns=[\"rating\", \"prediction\", \"popularity\"],\n",
    "    categorical_columns=[\"user_id\", \"item_id\"],\n",
    "    ranking=[Recsys(\n",
    "        user_id=\"user_id\",\n",
    "        item_id=\"item_id\", \n",
    "        prediction=\"prediction\",\n",
    "        target=\"rating\"\n",
    "    )]\n",
    ")\n",
    "\n",
    "popularity_train_dataset = Dataset.from_pandas(\n",
    "    popularity_train_df,\n",
    "    data_definition=popularity_train_data_definition\n",
    ")\n",
    "\n",
    "# Create current data with popularity features\n",
    "popularity_current_data = current_data.copy()\n",
    "popularity_current_data['popularity'] = popularity_current_data['item_id'].map(item_popularity)\n",
    "popularity_current_data['popularity'] = popularity_current_data['popularity'].fillna(\n",
    "    popularity_current_data['popularity'].median()\n",
    ")\n",
    "\n",
    "popularity_reference_data = reference_data.copy()\n",
    "popularity_reference_data['popularity'] = popularity_reference_data['item_id'].map(item_popularity)\n",
    "popularity_reference_data['popularity'] = popularity_reference_data['popularity'].fillna(\n",
    "    popularity_reference_data['popularity'].median()\n",
    ")\n",
    "\n",
    "# Create data definition with popularity column\n",
    "popularity_data_definition = DataDefinition(\n",
    "    numerical_columns=[\"rating\", \"prediction\", \"popularity\"],\n",
    "    categorical_columns=[\"user_id\", \"item_id\"],\n",
    "    ranking=[Recsys(\n",
    "        user_id=\"user_id\",\n",
    "        item_id=\"item_id\", \n",
    "        prediction=\"prediction\",\n",
    "        target=\"rating\"\n",
    "    )]\n",
    ")\n",
    "\n",
    "popularity_current_dataset = Dataset.from_pandas(\n",
    "    popularity_current_data,\n",
    "    data_definition=popularity_data_definition\n",
    ")\n",
    "\n",
    "popularity_reference_dataset = Dataset.from_pandas(\n",
    "    popularity_reference_data,\n",
    "    data_definition=popularity_data_definition\n",
    ")\n",
    "\n",
    "# PopularityBiasMetric with realistic data\n",
    "popularity_bias_report = Report([\n",
    "    PopularityBiasMetric(k=10, normalize_arp=True)\n",
    "])\n",
    "\n",
    "popularity_bias_snapshot = popularity_bias_report.run(\n",
    "    popularity_current_dataset, \n",
    "    popularity_reference_dataset,\n",
    "    additional_data={\n",
    "        \"current_train_data\": popularity_train_dataset,  # Training data with realistic popularity\n",
    "        \"reference_train_data\": popularity_train_dataset\n",
    "    }\n",
    ")\n",
    "popularity_bias_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "novelty_example",
   "metadata": {},
   "source": [
    "# Novelty - measures how novel the recommendations are\n",
    "# Note: This requires training data with historical item interactions\n",
    "# Create proper training data with historical item popularity\n",
    "\n",
    "# Create training data with realistic item popularity distribution\n",
    "# This represents historical data used to train the model\n",
    "np.random.seed(42)  # For reproducibility\n",
    "n_items = len(current_data['item_id'].unique())\n",
    "n_users = len(current_data['user_id'].unique())\n",
    "\n",
    "# Create historical item popularity (some items much more popular than others)\n",
    "item_popularity = {}\n",
    "for i, item_id in enumerate(current_data['item_id'].unique()):\n",
    "    # Create power-law distribution: some items very popular, others rare\n",
    "    popularity = np.random.pareto(1.5) + 1  # Pareto distribution for realistic popularity\n",
    "    item_popularity[item_id] = popularity\n",
    "\n",
    "# Create training data with this popularity distribution\n",
    "novelty_train_data = []\n",
    "for user_id in current_data['user_id'].unique():\n",
    "    # Each user interacts with different number of items (some users more active)\n",
    "    n_interactions = np.random.poisson(5) + 1  # Average 6 interactions per user\n",
    "    user_items = np.random.choice(\n",
    "        list(item_popularity.keys()), \n",
    "        size=min(n_interactions, n_items), \n",
    "        replace=False\n",
    "    )\n",
    "    for item_id in user_items:\n",
    "        novelty_train_data.append({\n",
    "            'user_id': user_id,\n",
    "            'item_id': item_id,\n",
    "            'prediction': 0.5,  # Dummy prediction\n",
    "            'rating': 3.0,  # Dummy rating\n",
    "        })\n",
    "\n",
    "novelty_train_df = pd.DataFrame(novelty_train_data)\n",
    "\n",
    "# Create training dataset\n",
    "novelty_train_data_definition = DataDefinition(\n",
    "    numerical_columns=[\"rating\", \"prediction\"],\n",
    "    categorical_columns=[\"user_id\", \"item_id\"],\n",
    "    ranking=[Recsys(\n",
    "        user_id=\"user_id\",\n",
    "        item_id=\"item_id\", \n",
    "        prediction=\"prediction\",\n",
    "        target=\"rating\"\n",
    "    )]\n",
    ")\n",
    "\n",
    "novelty_train_dataset = Dataset.from_pandas(\n",
    "    novelty_train_df,\n",
    "    data_definition=novelty_train_data_definition\n",
    ")\n",
    "\n",
    "# Novelty metric with proper training data\n",
    "novelty_report = Report([\n",
    "    Novelty(k=10)\n",
    "])\n",
    "\n",
    "novelty_snapshot = novelty_report.run(\n",
    "    current_dataset, \n",
    "    reference_dataset,\n",
    "    additional_data={\n",
    "        \"current_train_data\": novelty_train_dataset,  # Historical training data for novelty calculation\n",
    "        \"reference_train_data\": novelty_train_dataset\n",
    "    }\n",
    ")\n",
    "novelty_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "serendipity_example",
   "metadata": {},
   "source": [
    "# Serendipity - measures how surprising the recommendations are\n",
    "# Note: This requires training data and item features\n",
    "serendipity_report = Report([\n",
    "    Serendipity(k=10, item_features=[\"genre_cluster\", \"release_decade\", \"rating_tier\"])\n",
    "])\n",
    "\n",
    "serendipity_snapshot = serendipity_report.run(\n",
    "    diversity_current_dataset, \n",
    "    diversity_reference_dataset,\n",
    "    additional_data={\n",
    "        \"current_train_data\": diversity_current_dataset,  # Training data for serendipity calculation\n",
    "        \"reference_train_data\": diversity_reference_dataset\n",
    "    }\n",
    ")\n",
    "serendipity_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4d84d502",
   "metadata": {},
   "source": [
    "## Using the RecsysPreset\n",
    "\n",
    "The RecsysPreset provides a comprehensive set of recommendation metrics in one go. It automatically includes all relevant metrics based on the available data.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8e8e9b4a",
   "metadata": {},
   "source": [
    "# Basic RecsysPreset with minimal configuration\n",
    "basic_preset_report = Report([\n",
    "    RecsysPreset(\n",
    "        k=10,\n",
    "        min_rel_score=3,\n",
    "        ranking_name=\"default\"\n",
    "    )\n",
    "])\n",
    "\n",
    "basic_preset_snapshot = basic_preset_report.run(current_dataset, reference_dataset)\n",
    "basic_preset_snapshot\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "90105362",
   "metadata": {},
   "source": [
    "# Comprehensive RecsysPreset with all features\n",
    "# Create comprehensive training data that includes all required features\n",
    "\n",
    "# Recreate item popularity for comprehensive training data\n",
    "np.random.seed(42)  # For reproducibility\n",
    "item_popularity = {}\n",
    "for i, item_id in enumerate(current_data['item_id'].unique()):\n",
    "    # Create power-law distribution: some items very popular, others rare\n",
    "    popularity = np.random.pareto(1.5) + 1  # Pareto distribution for realistic popularity\n",
    "    item_popularity[item_id] = popularity\n",
    "\n",
    "# Create comprehensive training data with all features needed by the preset\n",
    "comprehensive_train_data = []\n",
    "for user_id in current_data['user_id'].unique():\n",
    "    # Each user interacts with different number of items (some users more active)\n",
    "    n_interactions = np.random.poisson(5) + 1  # Average 6 interactions per user\n",
    "    user_items = np.random.choice(\n",
    "        list(item_popularity.keys()), \n",
    "        size=min(n_interactions, n_items), \n",
    "        replace=False\n",
    "    )\n",
    "    for item_id in user_items:\n",
    "        # Get item features for this item\n",
    "        item_features = item_features_df[item_features_df['item_id'] == item_id].iloc[0]\n",
    "        item_bias = item_bias_df[item_bias_df['item_id'] == item_id].iloc[0]\n",
    "        user_bias = user_bias_df[user_bias_df['user_id'] == user_id].iloc[0]\n",
    "        \n",
    "        # Convert popularity to numeric before adding to training data\n",
    "        popularity_mapping = {'Low': 1, 'Medium': 2, 'High': 3}\n",
    "        popularity_numeric = popularity_mapping[item_bias['popularity']]\n",
    "        \n",
    "        comprehensive_train_data.append({\n",
    "            'user_id': user_id,\n",
    "            'item_id': item_id,\n",
    "            'prediction': 0.5,  # Dummy prediction\n",
    "            'rating': 3.0,  # Dummy rating\n",
    "            'genre_cluster': item_features['genre_cluster'],\n",
    "            'release_decade': item_features['release_decade'],\n",
    "            'rating_tier': item_features['rating_tier'],\n",
    "            'popularity': popularity_numeric,  # Already numeric\n",
    "            'age_group': user_bias['age_group']\n",
    "        })\n",
    "\n",
    "comprehensive_train_df = pd.DataFrame(comprehensive_train_data)\n",
    "\n",
    "# Create comprehensive training dataset\n",
    "comprehensive_train_data_definition = DataDefinition(\n",
    "    numerical_columns=[\"rating\", \"prediction\", \"genre_cluster\", \"release_decade\", \"rating_tier\", \"popularity\"],\n",
    "    categorical_columns=[\"user_id\", \"item_id\", \"age_group\"],\n",
    "    ranking=[Recsys(\n",
    "        user_id=\"user_id\",\n",
    "        item_id=\"item_id\", \n",
    "        prediction=\"prediction\",\n",
    "        target=\"rating\"\n",
    "    )]\n",
    ")\n",
    "\n",
    "comprehensive_train_dataset = Dataset.from_pandas(\n",
    "    comprehensive_train_df,\n",
    "    data_definition=comprehensive_train_data_definition\n",
    ")\n",
    "\n",
    "comprehensive_preset_report = Report([\n",
    "    RecsysPreset(\n",
    "        k=10,\n",
    "        min_rel_score=3,\n",
    "        ranking_name=\"default\",\n",
    "        user_ids=[\"user_0\", \"user_1\", \"user_2\"],  # Specific users for RecCasesTable\n",
    "        display_features=[\"genre_cluster\", \"release_decade\", \"rating_tier\"],  # Features to display in RecCasesTable\n",
    "        item_features=[\"genre_cluster\", \"release_decade\", \"rating_tier\"],  # Item features for diversity metrics\n",
    "        item_bias_columns=[\"popularity\"],  # Item bias analysis\n",
    "        user_bias_columns=[\"age_group\"],  # User bias analysis\n",
    "        normalize_arp=True,  # Normalize ARP in popularity bias\n",
    "        beta=1.0  # Beta parameter for F-Beta score\n",
    "    )\n",
    "])\n",
    "\n",
    "# Create comprehensive current and reference datasets with all required columns\n",
    "# These need to include item features, bias features, etc. for the comprehensive preset\n",
    "\n",
    "# Create comprehensive current dataset\n",
    "comprehensive_current_data = diversity_current_data.copy()\n",
    "comprehensive_current_data = comprehensive_current_data.merge(item_bias_df, on='item_id', how='left')\n",
    "comprehensive_current_data = comprehensive_current_data.merge(user_bias_df, on='user_id', how='left')\n",
    "\n",
    "# Convert categorical popularity to numeric for current data\n",
    "popularity_mapping = {'Low': 1, 'Medium': 2, 'High': 3}\n",
    "comprehensive_current_data['popularity'] = comprehensive_current_data['popularity'].map(popularity_mapping)\n",
    "comprehensive_current_data['popularity'] = comprehensive_current_data['popularity'].fillna(comprehensive_current_data['popularity'].median())\n",
    "\n",
    "# Fill missing age_group values\n",
    "comprehensive_current_data['age_group'] = comprehensive_current_data['age_group'].fillna('Unknown')\n",
    "\n",
    "# Create comprehensive reference dataset\n",
    "comprehensive_reference_data = diversity_reference_data.copy()\n",
    "comprehensive_reference_data = comprehensive_reference_data.merge(item_bias_df, on='item_id', how='left')\n",
    "comprehensive_reference_data = comprehensive_reference_data.merge(user_bias_df, on='user_id', how='left')\n",
    "\n",
    "# Convert categorical popularity to numeric for reference data\n",
    "comprehensive_reference_data['popularity'] = comprehensive_reference_data['popularity'].map(popularity_mapping)\n",
    "comprehensive_reference_data['popularity'] = comprehensive_reference_data['popularity'].fillna(comprehensive_reference_data['popularity'].median())\n",
    "\n",
    "# Fill missing age_group values\n",
    "comprehensive_reference_data['age_group'] = comprehensive_reference_data['age_group'].fillna('Unknown')\n",
    "\n",
    "# Create comprehensive data definition\n",
    "comprehensive_data_definition = DataDefinition(\n",
    "    numerical_columns=[\"rating\", \"prediction\", \"genre_cluster\", \"release_decade\", \"rating_tier\", \"popularity\"],\n",
    "    categorical_columns=[\"user_id\", \"item_id\", \"age_group\"],\n",
    "    ranking=[Recsys(\n",
    "        user_id=\"user_id\",\n",
    "        item_id=\"item_id\", \n",
    "        prediction=\"prediction\",\n",
    "        target=\"rating\"\n",
    "    )]\n",
    ")\n",
    "\n",
    "# Create comprehensive datasets\n",
    "comprehensive_current_dataset = Dataset.from_pandas(\n",
    "    comprehensive_current_data,\n",
    "    data_definition=comprehensive_data_definition\n",
    ")\n",
    "\n",
    "comprehensive_reference_dataset = Dataset.from_pandas(\n",
    "    comprehensive_reference_data,\n",
    "    data_definition=comprehensive_data_definition\n",
    ")\n",
    "\n",
    "comprehensive_preset_snapshot = comprehensive_preset_report.run(\n",
    "    comprehensive_current_dataset, \n",
    "    comprehensive_reference_dataset,\n",
    "    additional_data={\n",
    "        \"current_train_data\": comprehensive_train_dataset,  # Use comprehensive training data\n",
    "        \"reference_train_data\": comprehensive_train_dataset\n",
    "    }\n",
    ")\n",
    "comprehensive_preset_snapshot"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e911bebb",
   "metadata": {},
   "source": [
    "## Metric Results Analysis\n",
    "\n",
    "Let's examine the structure of metric results to understand how to access the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ca5f5dc1",
   "metadata": {},
   "source": [
    "# Access metric results\n",
    "precision_result = precision_snapshot.context.get_metric_result(PrecisionTopK(k=10, min_rel_score=3))\n",
    "\n",
    "print(\"Precision@K Result Structure:\")\n",
    "print(f\"Type: {type(precision_result)}\")\n",
    "print(f\"Current value: {precision_result.value}\")\n",
    "try:\n",
    "    precision_reference = precision_snapshot.context.get_reference_metric_result(PrecisionTopK(k=10, min_rel_score=3).metric_id)\n",
    "    print(f\"Reference value: {precision_reference.value}\")\n",
    "except:\n",
    "    print(\"Reference value: None\")\n",
    "\n",
    "# Access the DataFrame values\n",
    "print(\"\\nCurrent Precision@K DataFrame:\")\n",
    "print(f\"Current value: {precision_result.value}\")\n",
    "    \n",
    "try:\n",
    "    precision_reference = precision_snapshot.context.get_reference_metric_result(PrecisionTopK(k=10, min_rel_score=3).metric_id)\n",
    "    print(f\"Reference value: {precision_reference.value}\")\n",
    "except:\n",
    "    print(\"Reference value: None\")\n",
    "    print(\"\\nReference Precision@K DataFrame:\")\n",
    "try:\n",
    "    precision_reference = precision_snapshot.context.get_reference_metric_result(PrecisionTopK(k=10, min_rel_score=3).metric_id)\n",
    "    print(f\"Reference value: {precision_reference.value}\")\n",
    "except:\n",
    "    print(\"Reference value: None\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f56d8efa",
   "metadata": {},
   "source": [
    "# Access single value results (like Personalization)\n",
    "personalization_result = personalization_snapshot.context.get_metric_result(Personalization(k=10))\n",
    "\n",
    "print(\"Personalization Result Structure:\")\n",
    "print(f\"Type: {type(personalization_result)}\")\n",
    "print(f\"Current value: {personalization_result.value}\")\n",
    "try:\n",
    "    personalization_reference = personalization_snapshot.context.get_reference_metric_result(Personalization(k=10).metric_id)\n",
    "    print(f\"Reference value: {personalization_reference.value}\")\n",
    "except:\n",
    "    print(\"Reference value: None\")\n",
    "\n",
    "# Access the single values\n",
    "print(f\"Current value: {personalization_result.value}\")\n",
    "print(f\"Current value: {personalization_result.value}\")\n",
    "    \n",
    "try:\n",
    "    personalization_reference = personalization_snapshot.context.get_reference_metric_result(Personalization(k=10).metric_id)\n",
    "    print(f\"Reference value: {personalization_reference.value}\")\n",
    "except:\n",
    "    print(\"Reference value: None\")\n",
    "try:\n",
    "    personalization_reference = personalization_snapshot.context.get_reference_metric_result(Personalization(k=10).metric_id)\n",
    "    print(f\"Reference value: {personalization_reference.value}\")\n",
    "except:\n",
    "    print(\"Reference value: None\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6968137a",
   "metadata": {},
   "source": [
    "# Access RecCasesTable results (DataframeValue with user_id column)\n",
    "rec_cases_result = rec_cases_snapshot.context.get_metric_result(RecCasesTable(user_ids=[\"user_0\", \"user_1\", \"user_2\"], display_features=[\"genre_cluster\", \"release_decade\", \"rating_tier\"]))\n",
    "\n",
    "print(\"RecCasesTable Result Structure:\")\n",
    "print(f\"Type: {type(rec_cases_result)}\")\n",
    "print(f\"Current value: {rec_cases_result.value}\")\n",
    "try:\n",
    "    rec_cases_reference = rec_cases_snapshot.context.get_reference_metric_result(RecCasesTable(user_ids=[\"user_0\", \"user_1\", \"user_2\"], display_features=[\"genre_cluster\", \"release_decade\", \"rating_tier\"]).metric_id)\n",
    "    print(f\"Reference value: {rec_cases_reference.value}\")\n",
    "except:\n",
    "    print(\"Reference value: None\")\n",
    "\n",
    "# Access the DataFrame values\n",
    "print(\"\\nCurrent RecCasesTable DataFrame:\")\n",
    "print(f\"Current value: {rec_cases_result.value}\")\n",
    "    \n",
    "try:\n",
    "    rec_cases_reference = rec_cases_snapshot.context.get_reference_metric_result(RecCasesTable(user_ids=[\"user_0\", \"user_1\", \"user_2\"], display_features=[\"genre_cluster\", \"release_decade\", \"rating_tier\"]).metric_id)\n",
    "    print(f\"Reference value: {rec_cases_reference.value}\")\n",
    "except:\n",
    "    print(\"Reference value: None\")\n",
    "    print(\"\\nReference RecCasesTable DataFrame:\")\n",
    "try:\n",
    "    rec_cases_reference = rec_cases_snapshot.context.get_reference_metric_result(RecCasesTable(user_ids=[\"user_0\", \"user_1\", \"user_2\"], display_features=[\"genre_cluster\", \"release_decade\", \"rating_tier\"]).metric_id)\n",
    "    print(f\"Reference value: {rec_cases_reference.value}\")\n",
    "except:\n",
    "    print(\"Reference value: None\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a3cc3233",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated how to use Evidently's recommendation systems metrics:\n",
    "\n",
    "### Key Features:\n",
    "1. **Top-K Metrics**: Return DataframeValue with rank and value columns showing performance at different K values\n",
    "2. **Single Value Metrics**: Return SingleValue for metrics like Personalization, Diversity, etc.\n",
    "3. **Bias Analysis**: Return DataframeValue with x, y columns for distribution analysis\n",
    "4. **RecCasesTable**: Return DataframeValue with user_id column for detailed case analysis\n",
    "\n",
    "### Data Requirements:\n",
    "- **Basic metrics**: User ID, Item ID, Prediction scores, Target ratings\n",
    "- **Diversity metrics**: Additional item features (e.g., genres)\n",
    "- **Bias metrics**: User/item bias features and training data\n",
    "- **RecCasesTable**: Optional user_ids and display_features\n",
    "\n",
    "### Preset Usage:\n",
    "- **RecsysPreset**: Automatically includes relevant metrics based on available data\n",
    "- **Conditional inclusion**: Metrics are added based on data availability (training data, features, etc.)\n",
    "\n",
    "### Result Access:\n",
    "- Use `.current` and `.reference` to access current and reference period results\n",
    "- Use `.value` to access the actual data (DataFrame for DataframeValue, number for SingleValue)\n",
    "- Results can be used for further analysis, visualization, or monitoring\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
