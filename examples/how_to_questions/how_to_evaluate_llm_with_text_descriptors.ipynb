{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29489ded",
   "metadata": {
    "id": "29489ded"
   },
   "source": [
    "# How to evaluate llm with text descriptors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997d80b6",
   "metadata": {
    "id": "997d80b6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import time\n",
    "from datetime import timedelta\n",
    "\n",
    "from sklearn import datasets, ensemble, model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcacbf00",
   "metadata": {
    "id": "dcacbf00"
   },
   "outputs": [],
   "source": [
    "from evidently.ui.workspace.cloud import CloudWorkspace\n",
    "\n",
    "from evidently import ColumnMapping\n",
    "from evidently.report import Report\n",
    "\n",
    "from evidently.metrics import ColumnSummaryMetric, ColumnDistributionMetric, ColumnDriftMetric, DataDriftTable, TextDescriptorsDistribution\n",
    "\n",
    "from evidently.metric_preset import DataDriftPreset, DataQualityPreset, TextOverviewPreset\n",
    "\n",
    "from evidently.descriptors import HuggingFaceModel, OpenAIPrompting \n",
    "from evidently.descriptors import RegExp, BeginsWith, EndsWith, Contains, DoesNotContain, IncludesWords, ExcludesWords\n",
    "from evidently.descriptors import TextLength, OOV, NonLetterCharacterPercentage, SentenceCount, WordCount, Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9640360",
   "metadata": {
    "id": "a9640360",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0dd05c-1a80-4d51-84dd-93f1776f1352",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcad6c8-9abb-4c66-aa58-4e370baef072",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_logs =  pd.read_csv('chat_df.csv', index_col=0, parse_dates=['start_time', 'end_time'])\n",
    "assistant_logs.index = assistant_logs.start_time\n",
    "assistant_logs.index.rename('index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f16eb76-9100-4a67-a1a3-bb836d4c4b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_logs[[\"question\", \"response\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f0677b-511d-4254-b6f8-972edd21e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_logs.iloc[6].question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db78b522-deab-489e-bd24-501c629d019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_logs.iloc[6].response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe64a1b-a445-40ab-8125-7a7cf5e703ae",
   "metadata": {},
   "source": [
    "# One-off reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de2ddab-24a9-49ef-a1c8-b3e5121efce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping = ColumnMapping(\n",
    "    datetime='start_time',\n",
    "    datetime_features=['end_time'],\n",
    "    text_features=['question', 'response'],\n",
    "    categorical_features=['organization', 'model_ID', 'region', 'environment', 'feedback'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9da172-14aa-4614-96df-9ba57170ae28",
   "metadata": {},
   "source": [
    "### Simple descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5c518a-e4b9-4911-879e-9fd008cab51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Built-in descriptors without parameters\n",
    "report = Report(metrics=[\n",
    "        ColumnSummaryMetric(column_name = Sentiment(display_name=\"Question sentiment\").for_column(\"question\")),\n",
    "        ColumnSummaryMetric(column_name = TextLength(display_name= \"Question length\").for_column(\"question\")),\n",
    "        ColumnSummaryMetric(column_name = OOV(display_name= \"Question out of vocabulary words\").for_column(\"question\")),\n",
    "        ColumnSummaryMetric(column_name = Sentiment(display_name=\"Response sentiment\").for_column(\"response\")),\n",
    "        ColumnSummaryMetric(column_name = NonLetterCharacterPercentage(display_name=\"Non letter characters in response\").for_column(\"response\")),\n",
    "        ColumnSummaryMetric(column_name = SentenceCount(display_name=\"Sentence count in response\").for_column(\"response\")),\n",
    "        ColumnSummaryMetric(column_name = WordCount(display_name=\"Word count in response\").for_column(\"response\")),\n",
    "])\n",
    "\n",
    "report.run(reference_data=assistant_logs[datetime(2024, 4, 8) : datetime(2024, 4, 9)], \n",
    "           current_data=assistant_logs[datetime(2024, 4, 9) : datetime(2024, 4, 10)], \n",
    "           column_mapping=column_mapping)\n",
    "report    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad83213-f387-40c3-81d4-6c8c428b85cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Built-in descriptors with parameters\n",
    "report = Report(metrics=[\n",
    "        ColumnSummaryMetric(column_name = BeginsWith(display_name=\"'How' question\", prefix=\"How\").for_column(\"question\")),\n",
    "        ColumnSummaryMetric(column_name = EndsWith(display_name=\"Assisrance might be needed\", suffix=\"for assistance.\").for_column(\"response\")),\n",
    "        ColumnSummaryMetric(column_name = RegExp(reg_exp=r\"^I\", display_name= \"Question begins with 'I'\").for_column(\"question\")), \n",
    "        ColumnSummaryMetric(column_name = IncludesWords(words_list=['invoice', 'salary'],\n",
    "                                                               display_name=\"Questions about invoices and salary\").for_column(\"question\")),\n",
    "        ColumnSummaryMetric(column_name = ExcludesWords(words_list=['wrong', 'mistake'], \n",
    "                                                               display_name=\"Responses without mention of mistakes\").for_column(\"response\")),\n",
    "        ColumnSummaryMetric(column_name = Contains(items=['medical leave'], \n",
    "                                                               display_name=\"contains 'medical leave'\").for_column(\"response\")),\n",
    "        ColumnSummaryMetric(column_name = DoesNotContain(items=['employee portal'], \n",
    "                                                               display_name=\"does not contain 'employee portal'\").for_column(\"response\")),\n",
    "])\n",
    "\n",
    "report.run(reference_data=assistant_logs[datetime(2024, 4, 8) : datetime(2024, 4, 9)], \n",
    "           current_data=assistant_logs[datetime(2024, 4, 9) : datetime(2024, 4, 10)], \n",
    "           column_mapping=column_mapping)\n",
    "report    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b0f0fb-80eb-436e-83d5-508f274b4f41",
   "metadata": {},
   "source": [
    "### LLM-based descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce02433-b1c4-4c6f-8bec-73ed5ea01d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pii_prompt = \"\"\"\n",
    "Personally identifiable information (PII) is information that, when used alone or with other relevant data, can identify an individual.\n",
    "\n",
    "PII may contain direct identifiers (e.g., passport information) that can identify a person uniquely, \n",
    "or quasi-identifiers (e.g., race) that can be combined with other quasi-identifiers (e.g., date of birth) to successfully recognize an individual.\n",
    "PII may contain person's name, person's address,and something I may forget to mention\n",
    "\n",
    "Please identify whether or not the above text contains PII\n",
    "\n",
    "text: REPLACE \n",
    "\n",
    "Use the following categories for PII identification:\n",
    "1 if text contains PII\n",
    "0 if text does not contain PII\n",
    "0 if the information provided is not sufficient to make a clear determination\n",
    "\n",
    "Retrun a category only\n",
    "\"\"\"\n",
    "\n",
    "negativity_prompt = \"\"\"\n",
    "Classify text into two groups: negative and positive\n",
    "\n",
    "text: REPLACE \n",
    "\n",
    "Use the following categories for classification:\n",
    "NEGATIVE if text is negative\n",
    "POSITIVE if text is NOT negative\n",
    "UNKNOWN use this category only if the information provided is not sufficient to make a clear determination\n",
    "\n",
    "Retrun only category\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac74c17-7bb7-448e-96a0-af8898225cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descriptors with external models\n",
    "#to run OpenAIPrompting descriptor make sure you set environement variable with openai token \n",
    "report = Report(metrics=[\n",
    "        ColumnSummaryMetric(column_name = HuggingFaceModel(\"toxicity\", \"DaNLP/da-electra-hatespeech-detection\", {\"module_type\": \"measurement\"}, {\"toxic_label\": \"offensive\"}, \"toxicity\", display_name=\"Hugging Face Toxicity for response\").for_column(\"response\")),\n",
    "        ColumnSummaryMetric(column_name = OpenAIPrompting(prompt=pii_prompt, \n",
    "                                                      prompt_replace_string=\"REPLACE\", \n",
    "                                                      model=\"gpt-3.5-turbo-instruct\", \n",
    "                                                      feature_type=\"num\",\n",
    "                                                      display_name=\"PII for response (by gpt3.5)\").for_column(\"response\")),\n",
    "        ColumnSummaryMetric(column_name = OpenAIPrompting(prompt=negativity_prompt, \n",
    "                                                      prompt_replace_string=\"REPLACE\", \n",
    "                                                      model=\"gpt-3.5-turbo-instruct\", \n",
    "                                                      feature_type=\"cat\",\n",
    "                                                      display_name=\"Negativity for response (by gpt3.5)\").for_column(\"response\")),\n",
    "])\n",
    "\n",
    "report.run(reference_data=assistant_logs[datetime(2024, 4, 8) : datetime(2024, 4, 9)], \n",
    "           current_data=assistant_logs[datetime(2024, 4, 9) : datetime(2024, 4, 10)], \n",
    "           column_mapping=column_mapping)\n",
    "\n",
    "report    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4ac83b-4d07-4050-95fa-45009ab5aa1d",
   "metadata": {},
   "source": [
    "## Get dataset with calculated descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a549cc-79a2-4862-9a8b-994762ea40af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference dataset enriched with descriptors\n",
    "report.datasets()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e7de13-d278-4871-b32d-7934763c6712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#current dataset enriched with descriptors\n",
    "report.datasets()[1]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
