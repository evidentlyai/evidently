{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = fetch_openml(name='adult', version=2, as_frame='auto')\n",
    "df = data.frame\n",
    "\n",
    "ref = df[:20000]\n",
    "curr = df[20000:]\n",
    "\n",
    "curr['target'] = curr['education-num']\n",
    "curr['preds'] = curr['education-num'].values + np.random.normal(0, 6, curr.shape[0])\n",
    "ref['target'] = ref['education-num']\n",
    "ref['preds'] = ref['education-num'].values + np.random.normal(0, 6, ref.shape[0])\n",
    "\n",
    "curr.iloc[:2000, 3:5] = np.nan\n",
    "curr.iloc[:2000, 12] = np.nan\n",
    "curr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and run a report with separate data drift preset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from evidently import ColumnMapping\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import DataDrift\n",
    "\n",
    "\n",
    "report = Report(metrics=[\n",
    "    DataDrift(),\n",
    "])\n",
    "\n",
    "\n",
    "report.run(reference_data=ref, current_data=curr, column_mapping=ColumnMapping(target='target', prediction='preds'))\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.save_html('test_save.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.save_json('test_save.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently.metric_preset import classification_performance\n",
    "from evidently.metric_preset import regression_performance\n",
    "from evidently.metric_preset import data_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_report = Report(metrics=[\n",
    "    regression_performance.RegressionPerformance(),\n",
    "])\n",
    "\n",
    "\n",
    "regression_report.run(reference_data=ref, current_data=curr, \n",
    "                      column_mapping=ColumnMapping(target='target', prediction='preds'))\n",
    "regression_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_quality_report = Report(metrics=[\n",
    "    data_quality.DataQuality(),\n",
    "])\n",
    "\n",
    "\n",
    "data_quality_report.run(reference_data=ref, current_data=curr, \n",
    "                      column_mapping=ColumnMapping(target='target', prediction='preds'))\n",
    "data_quality_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_quality_report = Report(metrics=[\n",
    "    data_quality.DataQuality(),\n",
    "])\n",
    "\n",
    "\n",
    "data_quality_report.run(reference_data=ref, current_data=curr, \n",
    "                      column_mapping=ColumnMapping(target='target', prediction='preds'))\n",
    "data_quality_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from evidently import ColumnMapping\n",
    "from evidently.report import Report\n",
    "from evidently.metrics import DataIntegrityMetrics\n",
    "from evidently.metrics import ColumnRegExpMetric\n",
    "from evidently.metrics import DataIntegrityNullValuesMetrics\n",
    "from evidently.metrics import DataDriftTable\n",
    "from evidently.metrics import RegressionPerformanceMetrics\n",
    "from evidently.metrics import DataQualityMetrics\n",
    "from evidently.metrics import DataQualityStabilityMetrics\n",
    "from evidently.metrics import DataQualityValueListMetrics\n",
    "from evidently.metrics import DataQualityValueRangeMetrics\n",
    "from evidently.metrics import DataQualityValueQuantileMetrics\n",
    "from evidently.metrics import DataQualityCorrelationMetrics\n",
    "\n",
    "\n",
    "# make one feature drifted\n",
    "curr['education-num'] = 0\n",
    "data_mapping = ColumnMapping(target='target', prediction='preds')\n",
    "curr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = Report(metrics=[\n",
    "    DataDrift(),\n",
    "])\n",
    "\n",
    "report.run(reference_data=ref, current_data=curr, column_mapping=data_mapping)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = Report(metrics=[\n",
    "    DataDrift(),\n",
    "])\n",
    "error_message = 'No errors'\n",
    "\n",
    "try:\n",
    "    report.run(current_data=curr, reference_data=None, column_mapping=data_mapping)\n",
    "\n",
    "except ValueError as error:\n",
    "    error_message = error\n",
    "\n",
    "error_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = Report(metrics=[\n",
    "    DataDrift(),\n",
    "])\n",
    "error_message = 'No errors'\n",
    "\n",
    "try:\n",
    "    report.run(current_data=None, reference_data=ref, column_mapping=data_mapping)\n",
    "\n",
    "except ValueError as error:\n",
    "    error_message = error\n",
    "\n",
    "error_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = Report(metrics=[\n",
    "    RegressionPerformanceMetrics(),\n",
    "])\n",
    "\n",
    "report.run(reference_data=ref, current_data=curr, column_mapping=data_mapping)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = Report(metrics=[\n",
    "    RegressionPerformanceMetrics(),\n",
    "])\n",
    "\n",
    "report.run(current_data=curr, reference_data=None, column_mapping=data_mapping)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = Report(metrics=[\n",
    "    DataIntegrityMetrics(),\n",
    "    ColumnRegExpMetric(column_name=\"workclass\", reg_exp=r\".*-.*\"),\n",
    "    DataIntegrityNullValuesMetrics(),\n",
    "])\n",
    "\n",
    "report.run(reference_data=ref, current_data=curr, column_mapping=data_mapping)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.run(current_data=curr, reference_data=None, column_mapping=data_mapping)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = Report(metrics=[\n",
    "    DataQualityMetrics(),\n",
    "    DataQualityStabilityMetrics(),\n",
    "    DataQualityValueListMetrics(column_name=\"relationship\"),\n",
    "    DataQualityValueListMetrics(column_name=\"relationship\", values=[\"Not-in-family\", \"Unmarried\"]),\n",
    "    DataQualityValueRangeMetrics(column_name=\"age\", left=0, right=50),\n",
    "    DataQualityValueQuantileMetrics(column_name=\"age\", quantile=0.5),\n",
    "    DataQualityValueQuantileMetrics(column_name=\"age\", quantile=0.9),\n",
    "    DataQualityValueQuantileMetrics(column_name=\"age\", quantile=0.99),\n",
    "    DataQualityCorrelationMetrics()\n",
    "])\n",
    "\n",
    "report.run(current_data=curr, reference_data=ref, column_mapping=ColumnMapping(target='target', prediction='preds'))\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = Report(metrics=[\n",
    "    DataQualityMetrics(),\n",
    "    DataQualityStabilityMetrics(),\n",
    "    DataQualityValueListMetrics(column_name=\"relationship\", values=[\"Not-in-family\", \"Unmarried\"]),\n",
    "    DataQualityValueRangeMetrics(column_name=\"age\", left=0, right=50),\n",
    "    DataQualityValueQuantileMetrics(column_name=\"age\", quantile=0.5),\n",
    "    DataQualityValueQuantileMetrics(column_name=\"age\", quantile=0.9),\n",
    "    DataQualityValueQuantileMetrics(column_name=\"age\", quantile=0.99),\n",
    "    DataQualityCorrelationMetrics()\n",
    "])\n",
    "\n",
    "report.run(current_data=curr, reference_data=None, column_mapping=data_mapping)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary classification metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn import datasets\n",
    "from sklearn import ensemble\n",
    "from sklearn import model_selection\n",
    "\n",
    "from evidently.pipeline.column_mapping import ColumnMapping\n",
    "\n",
    "\n",
    "bcancer = datasets.load_breast_cancer()\n",
    "bcancer_frame = pd.DataFrame(bcancer.data, columns = bcancer.feature_names)\n",
    "bcancer_frame['target'] = bcancer.target\n",
    "target = 'target'\n",
    "prediction = 'prediction'\n",
    "\n",
    "numerical_features = bcancer.feature_names\n",
    "categorical_features = []\n",
    "\n",
    "features = numerical_features.tolist() + categorical_features\n",
    "train_data, test_data = model_selection.train_test_split(bcancer_frame, random_state=0)\n",
    "model = ensemble.RandomForestClassifier(random_state=0)\n",
    "model.fit(train_data[features], train_data.target)\n",
    "train_predictions = model.predict(train_data[features])\n",
    "test_predictions = model.predict(test_data[features])\n",
    "train_data['prediction'] = train_predictions\n",
    "test_data['prediction'] = test_predictions\n",
    "\n",
    "bcancer_column_mapping = ColumnMapping()\n",
    "bcancer_column_mapping.target = target\n",
    "bcancer_column_mapping.prediction = prediction\n",
    "bcancer_column_mapping.numerical_features = numerical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from evidently.report import Report\n",
    "from evidently.metrics import ClassificationPerformanceMetrics\n",
    "\n",
    "\n",
    "report = Report(metrics=[\n",
    "    ClassificationPerformanceMetrics(),\n",
    "])\n",
    "report.run(current_data=test_data, reference_data=train_data, column_mapping=bcancer_column_mapping)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.run(current_data=test_data, reference_data=None, column_mapping=bcancer_column_mapping)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prob classification metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = model_selection.train_test_split(bcancer_frame, random_state=0)\n",
    "model = ensemble.RandomForestClassifier(random_state=0)\n",
    "model.fit(train_data[features], train_data.target)\n",
    "\n",
    "train_probas = pd.DataFrame(model.predict_proba(train_data[features]))\n",
    "train_probas.columns = bcancer.target_names\n",
    "test_probas = pd.DataFrame(model.predict_proba(test_data[features]))\n",
    "test_probas.columns = bcancer.target_names\n",
    "\n",
    "# get labels for target: [0, 1, 0, 2] -> ['setosa', 'versicolor', 'setosa', 'virginica']\n",
    "\n",
    "train_data['target'] = [bcancer.target_names[x] for x in train_data['target']]\n",
    "test_data['target'] = [bcancer.target_names[x] for x in test_data['target']]\n",
    "\n",
    "# merge train and test data with predictions\n",
    "\n",
    "train_data.reset_index(inplace=True, drop=True)\n",
    "test_data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "merged_train_data = pd.concat([train_data, train_probas], axis=1)\n",
    "merged_test_data = pd.concat([test_data, test_probas], axis=1)\n",
    "\n",
    "bcancer_column_mapping = ColumnMapping()\n",
    "\n",
    "bcancer_column_mapping.target = target\n",
    "bcancer_column_mapping.prediction = bcancer.target_names.tolist()\n",
    "bcancer_column_mapping.numerical_features = numerical_features\n",
    "bcancer_column_mapping.pos_label = 'malignant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently.report import Report\n",
    "from evidently.metrics import ClassificationPerformanceMetrics\n",
    "from evidently.metrics import ClassificationPerformanceMetricsTopK\n",
    "from evidently.metrics import ClassificationPerformanceMetricsThreshold\n",
    "from evidently.metrics import DataQualityMetrics\n",
    "from evidently.metrics import DataIntegrityMetrics\n",
    "\n",
    "\n",
    "report = Report(metrics=[\n",
    "    #ClassificationPerformanceMetrics(),\n",
    "    #ClassificationPerformanceMetricsTopK(k=2),\n",
    "    ClassificationPerformanceMetricsThreshold(classification_threshold=0.4),\n",
    "])\n",
    "report.run(current_data=merged_test_data, reference_data=merged_train_data, column_mapping=bcancer_column_mapping)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.run(current_data=merged_test_data, reference_data=None, column_mapping=bcancer_column_mapping)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently.metric_preset import classification_performance\n",
    "report = Report(metrics=[\n",
    "    classification_performance.ClassificationPerformance(),\n",
    "])\n",
    "report.run(current_data=merged_test_data, reference_data=merged_train_data, column_mapping=bcancer_column_mapping)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
