{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = fetch_openml(name='adult', version=2, as_frame='auto')\n",
    "df = data.frame\n",
    "\n",
    "ref = df[:20000]\n",
    "curr = df[20000:]\n",
    "\n",
    "curr['target'] = curr['education-num']\n",
    "curr['preds'] = curr['education-num'].values + np.random.normal(0, 6, curr.shape[0])\n",
    "ref['target'] = ref['education-num']\n",
    "ref['preds'] = ref['education-num'].values + np.random.normal(0, 6, ref.shape[0])\n",
    "\n",
    "curr.iloc[:2000, 3:5] = np.nan\n",
    "curr.iloc[:2000, 12] = np.nan\n",
    "curr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and run a report with separate data drift preset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from evidently import ColumnMapping\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import DataDrift\n",
    "\n",
    "\n",
    "report = Report(metrics=[\n",
    "    DataDrift(),\n",
    "])\n",
    "\n",
    "\n",
    "report.run(reference_data=ref, current_data=curr, column_mapping=ColumnMapping(target='target', prediction='preds'))\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.save_html('test_save.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.save_json('test_save.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently.metric_preset import classification_performance\n",
    "from evidently.metric_preset import regression_performance\n",
    "from evidently.metric_preset import data_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_report = Report(metrics=[\n",
    "    regression_performance.RegressionPerformance(),\n",
    "])\n",
    "\n",
    "\n",
    "regression_report.run(reference_data=ref, current_data=curr, \n",
    "                      column_mapping=ColumnMapping(target='target', prediction='preds'))\n",
    "regression_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_quality_report = Report(metrics=[\n",
    "    data_quality.DataQuality(),\n",
    "])\n",
    "\n",
    "\n",
    "data_quality_report.run(reference_data=ref, current_data=curr, \n",
    "                      column_mapping=ColumnMapping(target='target', prediction='preds'))\n",
    "data_quality_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_quality_report = Report(metrics=[\n",
    "    data_quality.DataQuality(),\n",
    "])\n",
    "\n",
    "\n",
    "data_quality_report.run(reference_data=ref, current_data=curr, \n",
    "                      column_mapping=ColumnMapping(target='target', prediction='preds'))\n",
    "data_quality_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from evidently import ColumnMapping\n",
    "from evidently.report import Report\n",
    "from evidently.metrics.data_integrity_metrics import DataIntegrityMetrics\n",
    "from evidently.metrics.data_integrity_metrics import ColumnRegExpMetric\n",
    "from evidently.metrics.data_integrity_metrics import DataIntegrityNullValuesMetrics\n",
    "from evidently.metrics.data_drift.data_drift_table import DataDriftTable\n",
    "from evidently.metrics.regression_performance_metrics import RegressionPerformanceMetrics\n",
    "from evidently.metrics.data_quality_metrics import DataQualityMetrics\n",
    "from evidently.metrics.data_quality_metrics import DataQualityStabilityMetrics\n",
    "from evidently.metrics.data_quality_metrics import DataQualityValueListMetrics\n",
    "from evidently.metrics.data_quality_metrics import DataQualityValueRangeMetrics\n",
    "from evidently.metrics.data_quality_metrics import DataQualityValueQuantileMetrics\n",
    "from evidently.metrics.data_quality_metrics import DataQualityCorrelationMetrics\n",
    "\n",
    "\n",
    "# make one feature drifted\n",
    "curr['education-num'] = 0\n",
    "data_mapping = ColumnMapping(target='target', prediction='preds')\n",
    "curr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = Report(metrics=[\n",
    "    DataDrift(),\n",
    "])\n",
    "\n",
    "report.run(reference_data=ref, current_data=curr, column_mapping=data_mapping)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = Report(metrics=[\n",
    "    DataDrift(),\n",
    "])\n",
    "error_message = 'No errors'\n",
    "\n",
    "try:\n",
    "    report.run(current_data=curr, reference_data=None, column_mapping=data_mapping)\n",
    "\n",
    "except ValueError as error:\n",
    "    error_message = error\n",
    "\n",
    "error_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = Report(metrics=[\n",
    "    DataDrift(),\n",
    "])\n",
    "error_message = 'No errors'\n",
    "\n",
    "try:\n",
    "    report.run(current_data=None, reference_data=ref, column_mapping=data_mapping)\n",
    "\n",
    "except ValueError as error:\n",
    "    error_message = error\n",
    "\n",
    "error_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = Report(metrics=[\n",
    "    RegressionPerformanceMetrics(),\n",
    "])\n",
    "\n",
    "report.run(reference_data=ref, current_data=curr, column_mapping=data_mapping)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = Report(metrics=[\n",
    "    RegressionPerformanceMetrics(),\n",
    "])\n",
    "\n",
    "report.run(current_data=curr, reference_data=None, column_mapping=data_mapping)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = Report(metrics=[\n",
    "    DataIntegrityMetrics(),\n",
    "    ColumnRegExpMetric(column_name=\"workclass\", reg_exp=r\".*-.*\"),\n",
    "    DataIntegrityNullValuesMetrics(),\n",
    "])\n",
    "\n",
    "report.run(reference_data=ref, current_data=curr, column_mapping=data_mapping)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.run(current_data=curr, reference_data=None, column_mapping=data_mapping)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = Report(metrics=[\n",
    "    DataQualityMetrics(),\n",
    "    DataQualityStabilityMetrics(),\n",
    "    DataQualityValueListMetrics(column_name=\"relationship\"),\n",
    "    DataQualityValueListMetrics(column_name=\"relationship\", values=[\"Not-in-family\", \"Unmarried\"]),\n",
    "    DataQualityValueRangeMetrics(column_name=\"age\", left=0, right=50),\n",
    "    DataQualityValueQuantileMetrics(column_name=\"age\", quantile=0.5),\n",
    "    DataQualityValueQuantileMetrics(column_name=\"age\", quantile=0.9),\n",
    "    DataQualityValueQuantileMetrics(column_name=\"age\", quantile=0.99),\n",
    "    DataQualityCorrelationMetrics()\n",
    "])\n",
    "\n",
    "report.run(current_data=curr, reference_data=ref, column_mapping=ColumnMapping(target='target', prediction='preds'))\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = Report(metrics=[\n",
    "    DataQualityMetrics(),\n",
    "    DataQualityStabilityMetrics(),\n",
    "    DataQualityValueListMetrics(column_name=\"relationship\", values=[\"Not-in-family\", \"Unmarried\"]),\n",
    "    DataQualityValueRangeMetrics(column_name=\"age\", left=0, right=50),\n",
    "    DataQualityValueQuantileMetrics(column_name=\"age\", quantile=0.5),\n",
    "    DataQualityValueQuantileMetrics(column_name=\"age\", quantile=0.9),\n",
    "    DataQualityValueQuantileMetrics(column_name=\"age\", quantile=0.99),\n",
    "    DataQualityCorrelationMetrics()\n",
    "])\n",
    "\n",
    "report.run(current_data=curr, reference_data=None, column_mapping=data_mapping)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary classification metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn import datasets\n",
    "from sklearn import ensemble\n",
    "from sklearn import model_selection\n",
    "\n",
    "from evidently.pipeline.column_mapping import ColumnMapping\n",
    "\n",
    "\n",
    "bcancer = datasets.load_breast_cancer()\n",
    "bcancer_frame = pd.DataFrame(bcancer.data, columns = bcancer.feature_names)\n",
    "bcancer_frame['target'] = bcancer.target\n",
    "target = 'target'\n",
    "prediction = 'prediction'\n",
    "\n",
    "numerical_features = bcancer.feature_names\n",
    "categorical_features = []\n",
    "\n",
    "features = numerical_features.tolist() + categorical_features\n",
    "train_data, test_data = model_selection.train_test_split(bcancer_frame, random_state=0)\n",
    "model = ensemble.RandomForestClassifier(random_state=0)\n",
    "model.fit(train_data[features], train_data.target)\n",
    "train_predictions = model.predict(train_data[features])\n",
    "test_predictions = model.predict(test_data[features])\n",
    "train_data['prediction'] = train_predictions\n",
    "test_data['prediction'] = test_predictions\n",
    "\n",
    "bcancer_column_mapping = ColumnMapping()\n",
    "bcancer_column_mapping.target = target\n",
    "bcancer_column_mapping.prediction = prediction\n",
    "bcancer_column_mapping.numerical_features = numerical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from evidently.report import Report\n",
    "from evidently.metrics import ClassificationPerformanceMetrics\n",
    "\n",
    "\n",
    "report = Report(metrics=[\n",
    "    ClassificationPerformanceMetrics(),\n",
    "])\n",
    "report.run(current_data=test_data, reference_data=train_data, column_mapping=bcancer_column_mapping)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.run(current_data=test_data, reference_data=None, column_mapping=bcancer_column_mapping)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prob classification metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = model_selection.train_test_split(bcancer_frame, random_state=0)\n",
    "model = ensemble.RandomForestClassifier(random_state=0)\n",
    "model.fit(train_data[features], train_data.target)\n",
    "\n",
    "train_probas = pd.DataFrame(model.predict_proba(train_data[features]))\n",
    "train_probas.columns = bcancer.target_names\n",
    "test_probas = pd.DataFrame(model.predict_proba(test_data[features]))\n",
    "test_probas.columns = bcancer.target_names\n",
    "\n",
    "# get labels for target: [0, 1, 0, 2] -> ['setosa', 'versicolor', 'setosa', 'virginica']\n",
    "\n",
    "train_data['target'] = [bcancer.target_names[x] for x in train_data['target']]\n",
    "test_data['target'] = [bcancer.target_names[x] for x in test_data['target']]\n",
    "\n",
    "# merge train and test data with predictions\n",
    "\n",
    "train_data.reset_index(inplace=True, drop=True)\n",
    "test_data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "merged_train_data = pd.concat([train_data, train_probas], axis=1)\n",
    "merged_test_data = pd.concat([test_data, test_probas], axis=1)\n",
    "\n",
    "bcancer_column_mapping = ColumnMapping()\n",
    "\n",
    "bcancer_column_mapping.target = target\n",
    "bcancer_column_mapping.prediction = bcancer.target_names.tolist()\n",
    "bcancer_column_mapping.numerical_features = numerical_features\n",
    "bcancer_column_mapping.pos_label = 'malignant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently.report import Report\n",
    "from evidently.metrics import ClassificationPerformanceMetrics\n",
    "from evidently.metrics import ClassificationPerformanceMetricsTopK\n",
    "from evidently.metrics import ClassificationPerformanceMetricsThreshold\n",
    "from evidently.metrics import DataQualityMetrics\n",
    "from evidently.metrics import DataIntegrityMetrics\n",
    "\n",
    "\n",
    "report = Report(metrics=[\n",
    "    #ClassificationPerformanceMetrics(),\n",
    "    #ClassificationPerformanceMetricsTopK(k=2),\n",
    "    ClassificationPerformanceMetricsThreshold(classification_threshold=0.4),\n",
    "])\n",
    "report.run(current_data=merged_test_data, reference_data=merged_train_data, column_mapping=bcancer_column_mapping)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.run(current_data=merged_test_data, reference_data=None, column_mapping=bcancer_column_mapping)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently.metric_preset import classification_performance\n",
    "report = Report(metrics=[\n",
    "    classification_performance.ClassificationPerformance(),\n",
    "])\n",
    "report.run(current_data=merged_test_data, reference_data=merged_train_data, column_mapping=bcancer_column_mapping)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
