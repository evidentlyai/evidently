{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Monitoring setup for Bicycle Sharing Demand Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This notebook shows how you can use the Evidently to:\n",
    "* calculate prerformance and data drift for the model, performed as batch checks \n",
    "* log models quality & data drift using MLflow Tracking\n",
    "* explore the result \n",
    "\n",
    "More examples are avaliable in the github: https://github.com/evidentlyai/evidently/tree/main/examples\n",
    "\n",
    "Evidently docs: https://docs.evidentlyai.com/\n",
    "\n",
    "Join our Discord: https://discord.com/invite/xZjKRaNp8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import json\n",
    "\n",
    "from sklearn import datasets, ensemble, model_selection\n",
    "from scipy.stats import anderson_ksamp\n",
    "\n",
    "from evidently.dashboard import Dashboard\n",
    "from evidently.pipeline.column_mapping import ColumnMapping\n",
    "from evidently.dashboard.tabs import DataDriftTab, NumTargetDriftTab, RegressionPerformanceTab\n",
    "from evidently.options import DataDriftOptions\n",
    "from evidently.model_profile import Profile\n",
    "from evidently.model_profile.sections import DataDriftProfileSection, RegressionPerformanceProfileSection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bicycle Demand Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "More information about the dataset can be found in UCI machine learning repository: https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset\n",
    "\n",
    "Acknowledgement: Fanaee-T, Hadi, and Gama, Joao, 'Event labeling combining ensemble detectors and background knowledge', Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "content = requests.get(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\").content\n",
    "with zipfile.ZipFile(io.BytesIO(content)) as arc:\n",
    "    raw_data = pd.read_csv(arc.open(\"hour.csv\"), header=0, sep=',', parse_dates=['dteday']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "raw_data.index = raw_data.apply(lambda row: datetime.datetime.combine(row.dteday.date(), datetime.time(row.hr)),\n",
    "                                axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target = 'cnt'\n",
    "prediction = 'prediction'\n",
    "numerical_features = ['temp', 'atemp', 'hum', 'windspeed', 'mnth', 'hr', 'weekday']\n",
    "categorical_features = ['season', 'holiday', 'workingday', ]#'weathersit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reference = raw_data.loc['2011-01-01 00:00:00':'2011-01-28 23:00:00']\n",
    "current = raw_data.loc['2011-01-29 00:00:00':'2011-02-28 23:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    reference[numerical_features + categorical_features],\n",
    "    reference[target],\n",
    "    test_size=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "regressor = ensemble.RandomForestRegressor(random_state = 0, n_estimators = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preds_train = regressor.predict(X_train)\n",
    "preds_test = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train['target'] = y_train\n",
    "X_train['prediction'] = preds_train\n",
    "\n",
    "X_test['target'] = y_test\n",
    "X_test['prediction'] = preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "column_mapping = ColumnMapping()\n",
    "\n",
    "column_mapping.target = 'target'\n",
    "column_mapping.prediction = 'prediction'\n",
    "column_mapping.numerical_features = numerical_features\n",
    "column_mapping.categorical_features = categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "regression_perfomance_dashboard = Dashboard(tabs=[RegressionPerformanceTab()])\n",
    "regression_perfomance_dashboard.calculate(X_train.sort_index(), X_test.sort_index(), \n",
    "                                          column_mapping=column_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regression_perfomance_dashboard.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Production model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "regressor.fit(reference[numerical_features + categorical_features], reference[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "column_mapping = ColumnMapping()\n",
    "\n",
    "column_mapping.target = target\n",
    "column_mapping.prediction = prediction\n",
    "column_mapping.numerical_features = numerical_features\n",
    "column_mapping.categorical_features = categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ref_prediction = regressor.predict(reference[numerical_features + categorical_features])\n",
    "reference['prediction'] = ref_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "regression_perfomance_dashboard = Dashboard(tabs=[RegressionPerformanceTab(verbose_level=0)])\n",
    "regression_perfomance_dashboard.calculate(reference, None, column_mapping=column_mapping)\n",
    "regression_perfomance_dashboard.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Some time has passed - how is the model working?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "current_prediction = regressor.predict(current[numerical_features + categorical_features])\n",
    "current['prediction'] = current_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Week 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regression_perfomance_dashboard.calculate(reference, current.loc['2011-01-29 00:00:00':'2011-02-07 23:00:00'], \n",
    "                                            column_mapping=column_mapping)\n",
    "regression_perfomance_dashboard.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Week 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "RegressionPerformanceTab.list_widgets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regression_perfomance_dashboard = Dashboard(tabs=[RegressionPerformanceTab(include_widgets=[\n",
    "    'Regression Model Performance Report.',\n",
    "    'Reference: Model Quality (+/- std)',\n",
    "    'Current: Model Quality (+/- std)',\n",
    "    'Current: Error (Predicted - Actual)',\n",
    "    'Current: Error Distribution',\n",
    "])])\n",
    "regression_perfomance_dashboard.calculate(reference, current.loc['2011-02-07 00:00:00':'2011-02-14 23:00:00'], \n",
    "                                            column_mapping=column_mapping)\n",
    "regression_perfomance_dashboard.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Week 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "regression_perfomance_dashboard.calculate(reference, current.loc['2011-02-15 00:00:00':'2011-02-21 23:00:00'], \n",
    "                                            column_mapping=column_mapping)\n",
    "regression_perfomance_dashboard.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Why the quality has dropped?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "column_mapping_drift = ColumnMapping()\n",
    "\n",
    "column_mapping_drift.target = target\n",
    "column_mapping_drift.prediction = prediction\n",
    "column_mapping_drift.numerical_features = numerical_features\n",
    "column_mapping_drift.categorical_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_drift_dashboard = Dashboard(tabs=[DataDriftTab()])\n",
    "data_drift_dashboard.calculate(reference,\n",
    "                               current.loc['2011-02-14 00:00:00':'2011-02-21 23:00:00'], \n",
    "                               column_mapping_drift\n",
    "                              )\n",
    "data_drift_dashboard.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Let's customize the report!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from evidently.calculations.stattests import StatTest\n",
    "\n",
    "def _anderson_stat_test(reference_data: pd.Series, current_data: pd.Series, feature_type: str, threshold: float):\n",
    "    p_value = anderson_ksamp(np.array([reference_data, current_data]))[2]\n",
    "    return p_value, p_value < threshold\n",
    "\n",
    "anderson_stat_test = StatTest(\n",
    "    name=\"anderson\",\n",
    "    display_name=\"Anderson test (p_value)\",\n",
    "    func=_anderson_stat_test,\n",
    "    allowed_feature_types=[\"num\"]\n",
    ")\n",
    "\n",
    "options = DataDriftOptions(feature_stattest_func=anderson_stat_test, nbinsx=20, confidence=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "the_dashboard = Dashboard(\n",
    "    tabs=[RegressionPerformanceTab(include_widgets=[\n",
    "                                    'Regression Model Performance Report.',\n",
    "                                    'Reference: Model Quality (+/- std)',\n",
    "                                    'Current: Model Quality (+/- std)',\n",
    "                                    'Current: Error (Predicted - Actual)',\n",
    "                                    'Current: Error Distribution',]\n",
    "                                  ),\n",
    "          DataDriftTab()],\n",
    "    options=[options])\n",
    "                                \n",
    "the_dashboard.calculate(reference,\n",
    "                        current.loc['2011-02-14 00:00:00':'2011-02-21 23:00:00'], \n",
    "                        column_mapping_drift)\n",
    "\n",
    "the_dashboard.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Jupyter vs Automated Run?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To run this part of the tutorial you need to install MLflow (if not installed yet)\n",
    "\n",
    "You can install MLflow with the following command: `pip install mlflow` or install MLflow with scikit-learn via `pip install mlflow[extras]`\n",
    "\n",
    "More details:https://mlflow.org/docs/latest/tutorials-and-examples/tutorial.html#id5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "#import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "experiment_batches = [\n",
    "    ('2011-01-29 00:00:00','2011-02-07 23:00:00'),\n",
    "    ('2011-02-07 00:00:00','2011-02-14 23:00:00'),\n",
    "    ('2011-02-15 00:00:00','2011-02-21 23:00:00'),  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_profile = Profile(sections=[DataDriftProfileSection(), RegressionPerformanceProfileSection()])\n",
    "model_profile.calculate(reference, \n",
    "                        current.loc[experiment_batches[0][0]:experiment_batches[0][1]],\n",
    "                        column_mapping=column_mapping_drift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "logged_json_profile = json.loads(model_profile.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logged_json_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logged_json_profile[\"regression_performance\"][\"data\"][\"metrics\"][\"current\"][\"mean_error\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logged_json_profile[\"data_drift\"][\"data\"][\"metrics\"][\"share_drifted_features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#log into MLflow\n",
    "client = MlflowClient()\n",
    "\n",
    "#set experiment\n",
    "mlflow.set_experiment('Model Quality Evaluation')\n",
    "\n",
    "#generate model profile\n",
    "model_profile = Profile(sections=[DataDriftProfileSection(), RegressionPerformanceProfileSection()])\n",
    "\n",
    "#start new run\n",
    "for date in experiment_batches:\n",
    "    with mlflow.start_run() as run: #inside brackets run_name='test'\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"begin\", date[0])\n",
    "        mlflow.log_param(\"end\", date[1])\n",
    "\n",
    "        # Get metrics\n",
    "        model_profile.calculate(reference, \n",
    "                        current.loc[date[0]:date[1]],\n",
    "                        column_mapping=column_mapping_drift)\n",
    "        logged_json_profile = json.loads(model_profile.json())\n",
    "        \n",
    "        me = logged_json_profile[\"regression_performance\"][\"data\"][\"metrics\"][\"current\"][\"mean_error\"]\n",
    "        mae = logged_json_profile[\"regression_performance\"][\"data\"][\"metrics\"][\"current\"][\"mean_abs_error\"]\n",
    "        drift_share = logged_json_profile[\"data_drift\"][\"data\"][\"metrics\"][\"share_drifted_features\"]\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric('me', round(me, 3))\n",
    "        mlflow.log_metric('mae', round(mae, 3))\n",
    "        mlflow.log_metric('drift_share', round(drift_share, 3))\n",
    "\n",
    "        print(run.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#run MLflow UI (NOT recommented! It will be more convinient to run it directly from the TERMINAL)\n",
    "#!mlflow ui"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}