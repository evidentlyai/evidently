{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91203a9bfbebbe7f",
   "metadata": {},
   "source": [
    "# üìä Evaluating LLM Outputs with Evidently Descriptors: A Cross-Provider Tutorial\n",
    "\n",
    "## üìù Overview\n",
    "\n",
    "In this tutorial, we'll explore how to run **Evidently descriptors** to generate data columns with different Large Language Model (LLM) providers. Evidently is a powerful open-source tool for monitoring, testing, and analyzing machine learning models ‚Äî and its descriptors make it easy to quantify and compare text generation metrics like length, sentiment, toxicity, and more.\n",
    "\n",
    "We'll set up a workflow where we:\n",
    "- üîå Connect to multiple LLM providers (like OpenAI, Ollama, Vertex)\n",
    "- üéõÔ∏è Run the same descriptors across different models\n",
    "- üìä Compare results side-by-side\n",
    "\n",
    "Whether you're assessing model performance, tuning prompt engineering strategies, or building monitoring for production LLM systems ‚Äî this notebook will give you a hands-on guide.\n",
    "\n",
    "## üìö What You‚Äôll Need\n",
    "- Python 3.10+\n",
    "- [Evidently](https://evidentlyai.com/) installed with extra `[llm]`\n",
    "- Access to one or more LLM providers (API keys or local models like Ollama)\n",
    "- Basic familiarity with Python and JSON\n",
    "\n",
    "## üöÄ Let‚Äôs get started!\n",
    "\n",
    "üìä First, let's prepare a simple dataset for negativity evaluation.\n",
    "\n",
    "We'll use a tiny sample of two text reviews to test how different LLM providers assess negativity.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\"review\": [\n",
    "    \"Your service is bad\",\n",
    "    \"Your service is good\",\n",
    "]})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7363bf9fdd4898a6",
   "metadata": {},
   "source": [
    "# üîå OpenAI Integration\n",
    "\n",
    "In this section, we'll run the **Negativity** descriptor using **OpenAI's GPT-4o-mini** model.\n",
    "To proceed, you'll need an **OpenAI API key**.\n",
    "\n",
    "You can provide it in two ways:\n",
    "- Set it as an environment variable: `OPENAI_API_KEY`\n",
    "- Or pass it directly via `OpenAIOptions`\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "512aeac39c7caf60",
   "metadata": {},
   "source": [
    "openai_api_key = \"...\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "65dc39daf8c20973",
   "metadata": {},
   "source": [
    "from evidently.descriptors import NegativityLLMEval\n",
    "from evidently import Dataset\n",
    "from evidently.llm.options import OpenAIOptions\n",
    "\n",
    "open_ai_negativity = NegativityLLMEval(\"review\", provider=\"openai\", model=\"gpt-4o-mini\")\n",
    "dataset = Dataset.from_pandas(df, descriptors=[open_ai_negativity], options=OpenAIOptions(api_key=openai_api_key))\n",
    "dataset.as_dataframe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b9f5ea6604d62f4a",
   "metadata": {},
   "source": [
    "# üîå Gemini Integration\n",
    "\n",
    "Now, let's switch to **Gemini 2.0 Flash**.\n",
    "Just like with OpenAI, you need to provide an API key and adjust the options.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "215a2cdc711c42ae",
   "metadata": {},
   "source": [
    "from evidently.llm.options import GeminiOptions\n",
    "\n",
    "gemini_api_key = \"...\"\n",
    "gemini_ai_negativity = NegativityLLMEval(\"review\", provider=\"gemini\", model=\"gemini-2.0-flash\")\n",
    "dataset = Dataset.from_pandas(df, descriptors=[gemini_ai_negativity], options=GeminiOptions(api_key=gemini_api_key))\n",
    "dataset.as_dataframe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üîå Vertex AI Integration\n",
    "\n",
    "We can also call Gemini models from Vertex AI.\n",
    "For that, you'll need to provide credentials json as api_key.\n"
   ],
   "id": "d4b9a697802a7420"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "from evidently.llm.options import VertexAIOptions\n",
    "\n",
    "vertex_credentials = {...}\n",
    "vertex_credentials_json = json.dumps(vertex_credentials)\n",
    "vertex_ai_negativity = NegativityLLMEval(\"review\", provider=\"vertex_ai\", model=\"gemini-2.0-flash\")\n",
    "dataset = Dataset.from_pandas(df, descriptors=[vertex_ai_negativity], options=VertexAIOptions(api_key=vertex_credentials_json))\n",
    "dataset.as_dataframe()"
   ],
   "id": "8765d02677e36693",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üîå Mistral Integration\n",
    "\n",
    "Now, let's switch to **Mistral**.\n",
    "Just like with OpenAI and Vertex AI, you need to provide an API key and adjust the options.\n"
   ],
   "id": "be97b413cdcd847b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from evidently.llm.options import MistralOptions\n",
    "\n",
    "mistral_api_key = \"...\"\n",
    "mistral_negativity = NegativityLLMEval(\"review\", provider=\"mistral\", model=\"mistral-small-2503\")\n",
    "dataset = Dataset.from_pandas(df, descriptors=[mistral_negativity], options=MistralOptions(api_key=mistral_api_key))\n",
    "dataset.as_dataframe()"
   ],
   "id": "f61bd9435926c2c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d9983f23687e7a00",
   "metadata": {},
   "source": [
    "# üåê Other Providers\n",
    "\n",
    "Evidently supports a variety of other providers out of the box.\n",
    "You can check which options classes are available by inspecting:\n",
    "\n",
    "`evidently.llm.options.__all__`\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "64a8b7833244fca4",
   "metadata": {},
   "source": [
    "from evidently.llm import options\n",
    "\n",
    "options.__all__"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6cf4044d1ec73dff",
   "metadata": {},
   "source": [
    "Even more providers are supported via the **legacy module**.\n",
    "Here's a list of them for reference:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "77a56fd3c043fcea",
   "metadata": {},
   "source": [
    "from evidently.legacy.utils.llm import wrapper\n",
    "\n",
    "for name in wrapper.__dict__:\n",
    "    if name.endswith(\"Options\"):\n",
    "        print(name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3f5bf5392decc141",
   "metadata": {},
   "source": [
    "Additionally, because Evidently relies on **LiteLLM** under the hood for API integration,\n",
    "you can access any model/provider supported by LiteLLM ‚Äî even if Evidently doesn't have a dedicated options class.\n",
    "\n",
    "Here's how to use the generic `LLMOptions` for this:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d12959a416e45963",
   "metadata": {},
   "source": [
    "from evidently.llm.options import LLMOptions\n",
    "\n",
    "litellm_openai_negativity = NegativityLLMEval(\"review\", provider=\"litellm\", model=\"openai/gpt-4o-mini\")\n",
    "dataset = Dataset.from_pandas(df, descriptors=[litellm_openai_negativity], options=LLMOptions(api_key=openai_api_key))\n",
    "dataset.as_dataframe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b953e4155413aa57",
   "metadata": {},
   "source": [
    "# üñ•Ô∏è Ollama: Running LLMs Locally\n",
    "\n",
    "You can also run models like **Llama 3.2** locally using **Ollama**.\n",
    "First, install it from [https://ollama.com/download](https://ollama.com/download).\n",
    "\n",
    "Then, pull and serve the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "88d6f95a2de13eda",
   "metadata": {},
   "source": [
    "! ollama pull llama3.2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bb69bc6529967e2c",
   "metadata": {},
   "source": [
    "! ollama serve"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5dd664356b5fc63d",
   "metadata": {},
   "source": "Check that the Ollama API is live:\n"
  },
  {
   "cell_type": "code",
   "id": "6a877a313aef68c1",
   "metadata": {},
   "source": [
    "! curl 127.0.0.1:11434"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let's run the negativity descriptor using the locally served Llama model.",
   "id": "d9ad500b37f98232"
  },
  {
   "cell_type": "code",
   "id": "2ad5677ea04817e",
   "metadata": {},
   "source": [
    "from evidently.llm.options import OllamaOptions\n",
    "\n",
    "ollama_negativity = NegativityLLMEval(\"review\", provider=\"ollama\", model=\"llama3.2\")\n",
    "dataset = Dataset.from_pandas(df, descriptors=[ollama_negativity], options=OllamaOptions(api_url=\"http://localhost:11434\"))\n",
    "dataset.as_dataframe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dc6dc78630815c9d",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Customizing LLM API Calls\n",
    "\n",
    "Evidently allows customizing API call parameters by subclassing the corresponding `Options` class.\n",
    "\n",
    "For example, to set a custom temperature for Ollama:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a1c36ad350d2ea18",
   "metadata": {},
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "class MyOllamaOptions(OllamaOptions):\n",
    "    api_url = \"http://localhost:11434\"\n",
    "    temperature: float = 0.7\n",
    "\n",
    "    def get_additional_kwargs(self) -> Dict[str, Any]:\n",
    "        return {\"temperature\": self.temperature}\n",
    "\n",
    "dataset = Dataset.from_pandas(df, descriptors=[ollama_negativity], options=MyOllamaOptions(temperature=0.3))\n",
    "dataset.as_dataframe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7631452f4a96169d",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
