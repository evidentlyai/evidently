{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91203a9bfbebbe7f",
   "metadata": {},
   "source": [
    "# ðŸ“Š Evaluating LLM Outputs with Evidently Descriptors: A Cross-Provider Tutorial\n",
    "\n",
    "## ðŸ“ Overview\n",
    "\n",
    "In this tutorial, we'll explore how to run **Evidently descriptors** to generate data columns with different Large Language Model (LLM) providers. Evidently is a powerful open-source tool for monitoring, testing, and analyzing machine learning models â€” and its descriptors make it easy to quantify and compare text generation metrics like length, sentiment, toxicity, and more.\n",
    "\n",
    "We'll set up a workflow where we:\n",
    "- ðŸ”Œ Connect to multiple LLM providers (like OpenAI, Ollama, Vertex)\n",
    "- ðŸŽ›ï¸ Run the same descriptors across different models\n",
    "- ðŸ“Š Compare results side-by-side\n",
    "\n",
    "Whether you're assessing model performance, tuning prompt engineering strategies, or building monitoring for production LLM systems â€” this notebook will give you a hands-on guide.\n",
    "\n",
    "## ðŸ“š What Youâ€™ll Need\n",
    "- Python 3.10+\n",
    "- [Evidently](https://evidentlyai.com/) installed with extra `[llm]`\n",
    "- Access to one or more LLM providers (API keys or local models like Ollama)\n",
    "- Basic familiarity with Python and JSON\n",
    "\n",
    "## ðŸš€ Letâ€™s get started!\n",
    "\n",
    "ðŸ“Š First, let's prepare a simple dataset for negativity evaluation.\n",
    "\n",
    "We'll use a tiny sample of two text reviews to test how different LLM providers assess negativity.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\"review\": [\n",
    "    \"Your service is bad\",\n",
    "    \"Your service is good\",\n",
    "]})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7363bf9fdd4898a6",
   "metadata": {},
   "source": [
    "# ðŸ”Œ OpenAI Integration\n",
    "\n",
    "In this section, we'll run the **Negativity** descriptor using **OpenAI's GPT-4o-mini** model.\n",
    "To proceed, you'll need an **OpenAI API key**.\n",
    "\n",
    "You can provide it in two ways:\n",
    "- Set it as an environment variable: `OPENAI_API_KEY`\n",
    "- Or pass it directly via `OpenAIOptions`\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "512aeac39c7caf60",
   "metadata": {},
   "source": [
    "openai_api_key = \"...\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "65dc39daf8c20973",
   "metadata": {},
   "source": [
    "from evidently.descriptors import NegativityLLMEval\n",
    "from evidently import Dataset\n",
    "from evidently.llm.options import OpenAIOptions\n",
    "\n",
    "open_ai_negativity = NegativityLLMEval(\"review\", provider=\"openai\", model=\"gpt-4o-mini\")\n",
    "dataset = Dataset.from_pandas(df, descriptors=[open_ai_negativity], options=OpenAIOptions(api_key=openai_api_key))\n",
    "dataset.as_dataframe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b9f5ea6604d62f4a",
   "metadata": {},
   "source": [
    "# ðŸ”Œ Gemini Integration\n",
    "\n",
    "Now, let's switch to **Gemini 2.0 Flash**.\n",
    "Just like with OpenAI, you need to provide an API key and adjust the options.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "215a2cdc711c42ae",
   "metadata": {},
   "source": [
    "from evidently.llm.options import GeminiOptions\n",
    "\n",
    "gemini_api_key = \"...\"\n",
    "gemini_ai_negativity = NegativityLLMEval(\"review\", provider=\"gemini\", model=\"gemini-2.0-flash\")\n",
    "dataset = Dataset.from_pandas(df, descriptors=[gemini_ai_negativity], options=GeminiOptions(api_key=gemini_api_key))\n",
    "dataset.as_dataframe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ðŸ”Œ Vertex AI Integration\n",
    "\n",
    "We can also call Gemini models from Vertex AI.\n",
    "For that, you'll need to provide credentials json as api_key.\n"
   ],
   "id": "d4b9a697802a7420"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "from evidently.llm.options import VertexAIOptions\n",
    "\n",
    "vertex_credentials = {...}\n",
    "vertex_credentials_json = json.dumps(vertex_credentials)\n",
    "vertex_ai_negativity = NegativityLLMEval(\"review\", provider=\"vertex_ai\", model=\"gemini-2.0-flash\")\n",
    "dataset = Dataset.from_pandas(df, descriptors=[vertex_ai_negativity], options=VertexAIOptions(api_key=vertex_credentials_json))\n",
    "dataset.as_dataframe()"
   ],
   "id": "8765d02677e36693",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ðŸ”Œ Mistral Integration\n",
    "\n",
    "Now, let's switch to **Mistral**.\n",
    "Just like with OpenAI and Vertex AI, you need to provide an API key and adjust the options.\n"
   ],
   "id": "be97b413cdcd847b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from evidently.llm.options import MistralOptions\n",
    "\n",
    "mistral_api_key = \"...\"\n",
    "mistral_negativity = NegativityLLMEval(\"review\", provider=\"mistral\", model=\"mistral-small-2503\")\n",
    "dataset = Dataset.from_pandas(df, descriptors=[mistral_negativity], options=MistralOptions(api_key=mistral_api_key))\n",
    "dataset.as_dataframe()"
   ],
   "id": "f61bd9435926c2c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d9983f23687e7a00",
   "metadata": {},
   "source": [
    "# ðŸŒ Other Providers\n",
    "\n",
    "Evidently supports a variety of other providers out of the box.\n",
    "You can check which options classes are available by inspecting:\n",
    "\n",
    "`evidently.llm.options.__all__`\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "64a8b7833244fca4",
   "metadata": {},
   "source": [
    "from evidently.llm import options\n",
    "\n",
    "options.__all__"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6cf4044d1ec73dff",
   "metadata": {},
   "source": [
    "Even more providers are supported via the **legacy module**.\n",
    "Here's a list of them for reference:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "77a56fd3c043fcea",
   "metadata": {},
   "source": [
    "from evidently.legacy.utils.llm import wrapper\n",
    "\n",
    "for name in wrapper.__dict__:\n",
    "    if name.endswith(\"Options\"):\n",
    "        print(name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3f5bf5392decc141",
   "metadata": {},
   "source": [
    "Additionally, because Evidently relies on **LiteLLM** under the hood for API integration,\n",
    "you can access any model/provider supported by LiteLLM â€” even if Evidently doesn't have a dedicated options class.\n",
    "\n",
    "Here's how to use the generic `LLMOptions` for this:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d12959a416e45963",
   "metadata": {},
   "source": [
    "from evidently.llm.options import LLMOptions\n",
    "\n",
    "litellm_openai_negativity = NegativityLLMEval(\"review\", provider=\"litellm\", model=\"openai/gpt-4o-mini\")\n",
    "dataset = Dataset.from_pandas(df, descriptors=[litellm_openai_negativity], options=LLMOptions(api_key=openai_api_key))\n",
    "dataset.as_dataframe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b953e4155413aa57",
   "metadata": {},
   "source": [
    "# ðŸ–¥ï¸ Ollama: Running LLMs Locally\n",
    "\n",
    "You can also run models like **Llama 3.2** locally using **Ollama**.\n",
    "First, install it from [https://ollama.com/download](https://ollama.com/download).\n",
    "\n",
    "Then, pull and serve the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "88d6f95a2de13eda",
   "metadata": {},
   "source": [
    "! ollama pull llama3.2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bb69bc6529967e2c",
   "metadata": {},
   "source": [
    "! ollama serve"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5dd664356b5fc63d",
   "metadata": {},
   "source": "Check that the Ollama API is live:\n"
  },
  {
   "cell_type": "code",
   "id": "6a877a313aef68c1",
   "metadata": {},
   "source": [
    "! curl 127.0.0.1:11434"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let's run the negativity descriptor using the locally served Llama model.",
   "id": "d9ad500b37f98232"
  },
  {
   "cell_type": "code",
   "id": "2ad5677ea04817e",
   "metadata": {},
   "source": [
    "from evidently.llm.options import OllamaOptions\n",
    "\n",
    "ollama_negativity = NegativityLLMEval(\"review\", provider=\"ollama\", model=\"llama3.2\")\n",
    "dataset = Dataset.from_pandas(df, descriptors=[ollama_negativity], options=OllamaOptions(api_url=\"http://localhost:11434\"))\n",
    "dataset.as_dataframe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dc6dc78630815c9d",
   "metadata": {},
   "source": [
    "# âš™ï¸ Customizing LLM API Calls\n",
    "\n",
    "Evidently allows customizing API call parameters by subclassing the corresponding `Options` class.\n",
    "\n",
    "For example, to set a custom temperature for Ollama:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a1c36ad350d2ea18",
   "metadata": {},
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "class MyOllamaOptions(OllamaOptions):\n",
    "    api_url = \"http://localhost:11434\"\n",
    "    temperature: float = 0.7\n",
    "\n",
    "    def get_additional_kwargs(self) -> Dict[str, Any]:\n",
    "        return {\"temperature\": self.temperature}\n",
    "\n",
    "dataset = Dataset.from_pandas(df, descriptors=[ollama_negativity], options=MyOllamaOptions(temperature=0.3))\n",
    "dataset.as_dataframe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7631452f4a96169d",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
